{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: mojimoji in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (0.0.13)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (0.2.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: click in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: colorama in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/17/1c/ccdd103cfcc9435a18819856fbbe0c20b8fa60bfc3343580de4be13f0668/scikit_learn-1.5.2-cp311-cp311-win_amd64.whl.metadata\n",
            "  Using cached scikit_learn-1.5.2-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
            "Collecting scipy>=1.6.0 (from scikit-learn)\n",
            "  Obtaining dependency information for scipy>=1.6.0 from https://files.pythonhosted.org/packages/ea/c2/5ecadc5fcccefaece775feadcd795060adf5c3b29a883bff0e678cfe89af/scipy-1.14.1-cp311-cp311-win_amd64.whl.metadata\n",
            "  Using cached scipy-1.14.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: joblib>=1.2.0 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
            "  Obtaining dependency information for threadpoolctl>=3.1.0 from https://files.pythonhosted.org/packages/4b/2c/ffbf7a134b9ab11a67b0cf0726453cedd9c5043a4fe7a35d1cefa9a1bcfb/threadpoolctl-3.5.0-py3-none-any.whl.metadata\n",
            "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
            "Using cached scikit_learn-1.5.2-cp311-cp311-win_amd64.whl (11.0 MB)\n",
            "Using cached scipy-1.14.1-cp311-cp311-win_amd64.whl (44.8 MB)\n",
            "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
            "Successfully installed scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting matplotlib\n",
            "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/8b/ce/15b0bb2fb29b3d46211d8ca740b96b5232499fc49200b58b8d571292c9a6/matplotlib-3.9.2-cp311-cp311-win_amd64.whl.metadata\n",
            "  Using cached matplotlib-3.9.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
            "Collecting contourpy>=1.0.1 (from matplotlib)\n",
            "  Obtaining dependency information for contourpy>=1.0.1 from https://files.pythonhosted.org/packages/8d/2f/804f02ff30a7fae21f98198828d0857439ec4c91a96e20cf2d6c49372966/contourpy-1.3.0-cp311-cp311-win_amd64.whl.metadata\n",
            "  Using cached contourpy-1.3.0-cp311-cp311-win_amd64.whl.metadata (5.4 kB)\n",
            "Collecting cycler>=0.10 (from matplotlib)\n",
            "  Obtaining dependency information for cycler>=0.10 from https://files.pythonhosted.org/packages/e7/05/c19819d5e3d95294a6f5947fb9b9629efb316b96de511b418c53d245aae6/cycler-0.12.1-py3-none-any.whl.metadata\n",
            "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting fonttools>=4.22.0 (from matplotlib)\n",
            "  Obtaining dependency information for fonttools>=4.22.0 from https://files.pythonhosted.org/packages/63/f1/3a081cd047d83b5966cb0d7ef3fea929ee6eddeb94d8fbfdb2a19bd60cc7/fonttools-4.54.1-cp311-cp311-win_amd64.whl.metadata\n",
            "  Using cached fonttools-4.54.1-cp311-cp311-win_amd64.whl.metadata (167 kB)\n",
            "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
            "  Obtaining dependency information for kiwisolver>=1.3.1 from https://files.pythonhosted.org/packages/a1/65/d43e9a20aabcf2e798ad1aff6c143ae3a42cf506754bcb6a7ed8259c8425/kiwisolver-1.4.7-cp311-cp311-win_amd64.whl.metadata\n",
            "  Using cached kiwisolver-1.4.7-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: numpy>=1.23 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from matplotlib) (24.1)\n",
            "Collecting pillow>=8 (from matplotlib)\n",
            "  Obtaining dependency information for pillow>=8 from https://files.pythonhosted.org/packages/dc/83/1470c220a4ff06cd75fc609068f6605e567ea51df70557555c2ab6516b2c/pillow-11.0.0-cp311-cp311-win_amd64.whl.metadata\n",
            "  Downloading pillow-11.0.0-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
            "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
            "  Obtaining dependency information for pyparsing>=2.3.1 from https://files.pythonhosted.org/packages/be/ec/2eb3cd785efd67806c46c13a17339708ddc346cbb684eade7a6e6f79536a/pyparsing-3.2.0-py3-none-any.whl.metadata\n",
            "  Downloading pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Using cached matplotlib-3.9.2-cp311-cp311-win_amd64.whl (7.8 MB)\n",
            "Using cached contourpy-1.3.0-cp311-cp311-win_amd64.whl (217 kB)\n",
            "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Using cached fonttools-4.54.1-cp311-cp311-win_amd64.whl (2.2 MB)\n",
            "Using cached kiwisolver-1.4.7-cp311-cp311-win_amd64.whl (56 kB)\n",
            "Downloading pillow-11.0.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
            "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.0/2.6 MB 653.6 kB/s eta 0:00:04\n",
            "   --- ------------------------------------ 0.2/2.6 MB 1.7 MB/s eta 0:00:02\n",
            "   -------- ------------------------------- 0.5/2.6 MB 3.3 MB/s eta 0:00:01\n",
            "   ------------------- -------------------- 1.3/2.6 MB 6.2 MB/s eta 0:00:01\n",
            "   ------------------------------ --------- 1.9/2.6 MB 7.7 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 2.1/2.6 MB 7.4 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 2.2/2.6 MB 6.6 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.6/2.6 MB 6.8 MB/s eta 0:00:00\n",
            "Downloading pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
            "   ---------------------------------------- 0.0/106.9 kB ? eta -:--:--\n",
            "   ---------------------------------------- 106.9/106.9 kB 6.4 MB/s eta 0:00:00\n",
            "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
            "Successfully installed contourpy-1.3.0 cycler-0.12.1 fonttools-4.54.1 kiwisolver-1.4.7 matplotlib-3.9.2 pillow-11.0.0 pyparsing-3.2.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install mojimoji\n",
        "!pip install sentencepiece\n",
        "!pip install nltk\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "colab_type": "code",
        "id": "jK88JoQ79a_G",
        "outputId": "915e7861-8684-4b8b-84df-0716a8e336a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.18.0\n",
            "WARNING:tensorflow:From C:\\Users\\prash\\AppData\\Local\\Temp\\ipykernel_13304\\450531839.py:30: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.config.list_physical_devices('GPU')` instead.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Input, dot, Activation, Concatenate\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras import initializers, regularizers, constraints\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "import mojimoji\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "\n",
        "import nltk\n",
        "import unicodedata\n",
        "import sentencepiece as spm\n",
        "\n",
        "# ignore warning\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "print(tf.__version__)\n",
        "# gpu\n",
        "tf.test.is_gpu_available() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J_L_ygdR9a_Q"
      },
      "source": [
        "# load text file\n",
        "\n",
        "this dataset is aleady implemented a SentenceSpace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jYlzMp1U9a_T"
      },
      "outputs": [],
      "source": [
        "num_example = 300000\n",
        "\n",
        "# create each languages list\n",
        "def create_lang_list(num_example):\n",
        "    # load txt file\n",
        "    lines =  io.open(\"datas/raw.txt\", encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "    word_pairs = [[w for w in l.split('\\t')]  for l in lines[:num_example]]\n",
        "\n",
        "    return zip(*word_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "qG4dRId39a_X"
      },
      "outputs": [],
      "source": [
        "en, ja = create_lang_list(num_example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MrWjSfDlABKb"
      },
      "source": [
        "**SentencePiece**\n",
        "\n",
        "using pretrain model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "t5WysbRO9-nj"
      },
      "outputs": [],
      "source": [
        "ja_sentence = list()\n",
        "for i in ja:\n",
        "    ja_sentence.append(i.replace(\" \", \"\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "J9M9slUl9_hh"
      },
      "outputs": [
        {
          "ename": "OSError",
          "evalue": "Not found: \"wiki-ja.model\": No such file or directory Error #2",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m ja_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n\u001b[0;32m      2\u001b[0m sp \u001b[38;5;241m=\u001b[39m spm\u001b[38;5;241m.\u001b[39mSentencePieceProcessor()\n\u001b[1;32m----> 3\u001b[0m \u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwiki-ja.model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m ja_sentence:\n\u001b[0;32m      5\u001b[0m     ja_text\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(sp\u001b[38;5;241m.\u001b[39mEncodeAsPieces(text))\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m▁\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip())\n",
            "File \u001b[1;32me:\\7th sem\\GO_GO_Nihongo\\venv\\Lib\\site-packages\\sentencepiece\\__init__.py:961\u001b[0m, in \u001b[0;36mSentencePieceProcessor.Load\u001b[1;34m(self, model_file, model_proto)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_proto:\n\u001b[0;32m    960\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLoadFromSerializedProto(model_proto)\n\u001b[1;32m--> 961\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_file\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32me:\\7th sem\\GO_GO_Nihongo\\venv\\Lib\\site-packages\\sentencepiece\\__init__.py:316\u001b[0m, in \u001b[0;36mSentencePieceProcessor.LoadFromFile\u001b[1;34m(self, arg)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLoadFromFile\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg):\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sentencepiece\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSentencePieceProcessor_LoadFromFile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mOSError\u001b[0m: Not found: \"wiki-ja.model\": No such file or directory Error #2"
          ]
        }
      ],
      "source": [
        "ja_text = list()\n",
        "sp = spm.SentencePieceProcessor()\n",
        "sp.Load(\"wiki-ja.model\")\n",
        "for text in ja_sentence:\n",
        "    ja_text.append(\" \".join(sp.EncodeAsPieces(text)).replace(\"▁\", \"\").strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "os6q5rD-9a_d"
      },
      "source": [
        "# Text Pre-processing\n",
        "\n",
        " **Removing accented characters**\n",
        " e.g. é → e.\n",
        " \n",
        "\n",
        "**remove special word**\n",
        "e.g. remove \"123#@!\"\n",
        "\n",
        "\n",
        "**normalize japanese**\n",
        "e.g.  カナダ  → ｶﾅﾀﾞ\n",
        "\n",
        "**Tokenize**\n",
        " e.g. I am going to restaurant → [[I], [am], [going], [to], [restaurant]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kZBfMYKbAR5w"
      },
      "source": [
        "# normalize Japanese (Hiragana and Katakana)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "tD17_NhQ-Dph"
      },
      "outputs": [],
      "source": [
        "def moji(text):\n",
        "  return mojimoji.zen_to_han(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "TN4FzAjRAhLk",
        "outputId": "af19ca4a-133a-43e6-8b97-fd3ee8aef4cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "カナダ → ｶﾅﾀﾞ\n"
          ]
        }
      ],
      "source": [
        "print(\"カナダ →\", moji(\"カナダ\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IlFoNcZK9a_d"
      },
      "source": [
        "# Removing accented characters\n",
        "\n",
        "English might have accent like é but Japanese doesn't have any accent\n",
        "I just create different function to ascii for Japanese and English"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "zBNtprYP9a_e"
      },
      "outputs": [],
      "source": [
        "# Removing accented characters\n",
        "def english_unicode_to_ascii(text):\n",
        "     return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore'))\n",
        "\n",
        "def japanese_unicode_to_ascii(text):\n",
        "    return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "Fed-yYrU9a_h",
        "outputId": "b11dad69-de38-495a-fdd7-27d49c8ea821"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('こんにちは。 今日は', 'Hello world e ')"
            ]
          },
          "execution_count": 17,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# e.g.\n",
        "japanese_unicode_to_ascii(\"こんにちは。 今日は\"), english_unicode_to_ascii(\"Hello world é \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ogJT2Kr9a_x"
      },
      "source": [
        "# remove special characters and create space between word and punctuation\n",
        "\n",
        "replacing everything with space except(a-z, A-Z, \".\",  \"?\",  \"!\",  \",\", \"-\", \"ー\", , \"。\", \"Kanji\", \"Katakana\", \"Hiragana\")\n",
        "create space between word and punctuation (? ! . , 、 。)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yiKfZSxd9a_x"
      },
      "outputs": [],
      "source": [
        "def replace_special_character_to_space_en(text):\n",
        "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
        "    text = re.sub(r'[\" \"]+', \" \", text)\n",
        "\n",
        "    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
        "\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "  \n",
        "def replace_special_character_to_space(text):\n",
        "    text = re.sub(r\"([?!。、¿])\", r\" \\1\", text)\n",
        "    pattern = r\"[^\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F?!\\s、。.,0-9]+\"\n",
        "    text = re.sub(pattern, '', text).rstrip().strip()\n",
        "    text = re.sub(r'[\" \"]+', \" \", text)\n",
        "    text = text.replace(\"・\" , \"\")\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "ifLQFqOU9a_2",
        "outputId": "3d2040da-bd55-43b5-dca6-f1a444f0f34b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('123.', 'こんにちはいい天気 。')"
            ]
          },
          "execution_count": 19,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# e.g.\n",
        "replace_special_character_to_space(\"hello #@…123world.\"), replace_special_character_to_space(\"こん・にちは・いい天気。\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "txwH8f9X9bAL"
      },
      "source": [
        "# Text normalize\n",
        "\n",
        "I will normalize English text and Japanese text using function which is defined so far"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "WLgyXVAy9bAN"
      },
      "outputs": [],
      "source": [
        "def normalize_english(english_text, japanese_text):\n",
        "    \n",
        "    input_value = []\n",
        "    target_value = []\n",
        "    \n",
        "    for en_text, ja_text in zip(english_text, japanese_text):\n",
        "        \n",
        "        # normalize English\n",
        "        en_text = english_unicode_to_ascii(en_text)\n",
        "        en_text = replace_special_character_to_space_en(en_text)\n",
        "\n",
        "        en_text = \"start_ \" + en_text + \" _end\"\n",
        "        # input value doesn't need  a START and END sentence  \n",
        "        input_value.append(en_text)\n",
        "\n",
        "        # normalize Japanese\n",
        "        ja_text = japanese_unicode_to_ascii(ja_text)\n",
        "        ja_text = replace_special_character_to_space(ja_text)\n",
        "        ja_text = moji(ja_text)\n",
        "\n",
        "        # add StTART and END sentence\n",
        "        ja_text = \"start_ \" + ja_text + \" _end\"\n",
        "        \n",
        "        target_value.append(ja_text)\n",
        "\n",
        "    return input_value, target_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Fv1iASn49bAR"
      },
      "outputs": [],
      "source": [
        "# get normalize text data\n",
        "input_value, target_value = normalize_english(en, ja_text)\n",
        "\n",
        "# convert to Series\n",
        "x = pd.Series(input_value) \n",
        "y = pd.Series(target_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "colab_type": "code",
        "id": "2-4jMOfx9bAW",
        "outputId": "3d804cbc-ef27-49bf-bb44-4f537697e32c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>input</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>start_ you are back , aren t you , harold ? _end</td>\n",
              "      <td>start_ あなた は 戻った の ね ﾊﾛﾙﾄ゙ ? _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>start_ my opponent is shark . _end</td>\n",
              "      <td>start_ 俺 の 相手 は ｼｬｰ ｸ だ ｡ _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>start_ this is one thing in exchange for anoth...</td>\n",
              "      <td>start_ 引き 換え だ ある 事 とある 物の _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>start_ yeah , i m fine . _end</td>\n",
              "      <td>start_ もう いい よ ご ち そう さま う うん _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>start_ don t come to the office anymore . don ...</td>\n",
              "      <td>start_ もう 会社 には 来 ない で くれ 電話 も する な _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>start_ looks beautiful . _end</td>\n",
              "      <td>start_ きれい だ ｡ _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>start_ get him out of here , because i will fu...</td>\n",
              "      <td>start_ 連れ て 行 け 殺し そう だ わ かった か ? _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>start_ you killed him ! _end</td>\n",
              "      <td>start_ 殺 した のか ! _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>start_ okay , then who ? _end</td>\n",
              "      <td>start_ わ ぁ ! いつも すみ ません ｡ いい の よ ｡ _end</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>start_ it seems a former employee . . . _end</td>\n",
              "      <td>start_ ｶﾝﾊ゚ﾆｰ の元 社員 が _end</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               input                                     target\n",
              "0   start_ you are back , aren t you , harold ? _end          start_ あなた は 戻った の ね ﾊﾛﾙﾄ゙ ? _end\n",
              "1                 start_ my opponent is shark . _end            start_ 俺 の 相手 は ｼｬｰ ｸ だ ｡ _end\n",
              "2  start_ this is one thing in exchange for anoth...           start_ 引き 換え だ ある 事 とある 物の _end\n",
              "3                      start_ yeah , i m fine . _end        start_ もう いい よ ご ち そう さま う うん _end\n",
              "4  start_ don t come to the office anymore . don ...  start_ もう 会社 には 来 ない で くれ 電話 も する な _end\n",
              "5                      start_ looks beautiful . _end                       start_ きれい だ ｡ _end\n",
              "6  start_ get him out of here , because i will fu...    start_ 連れ て 行 け 殺し そう だ わ かった か ? _end\n",
              "7                       start_ you killed him ! _end                      start_ 殺 した のか ! _end\n",
              "8                      start_ okay , then who ? _end    start_ わ ぁ ! いつも すみ ません ｡ いい の よ ｡ _end\n",
              "9       start_ it seems a former employee . . . _end                start_ ｶﾝﾊ゚ﾆｰ の元 社員 が _end"
            ]
          },
          "execution_count": 22,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame({\"input\": x, \"target\": y}).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZMlaIsar9bAf"
      },
      "source": [
        "# tokenize\n",
        "tokenize each language word based on space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ACN_sW7u9bAh"
      },
      "outputs": [],
      "source": [
        "def tokenize(lang):\n",
        "    # vectorize a text corpus\n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        filters=' ')\n",
        "\n",
        "    # updates internal vocabulary based on a list of texts\n",
        "    # e.g. \"[this place is good ]\"→{this:2, place:3, is:1, good:4} \"\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    # Transforms each text in texts to a sequence of integers.\n",
        "    # e.g. this place is good → [[2, 3, 1, 4]]\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "    # transform a list of num sample into a 2D Numpy array of shape \n",
        "    # Fixed length because length of sequence of integers are different\n",
        "    # return (len(sequences), maxlen)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                          padding='post')\n",
        "    return tensor, lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "colab_type": "code",
        "id": "JCZ7huXZ9bAs",
        "outputId": "6189376e-b127-4b80-85cf-f7d1162e4345"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([[ 2,  3,  1,  4],\n",
              "        [ 5,  6,  7,  8],\n",
              "        [ 9,  1, 10, 11]], dtype=int32),\n",
              " <keras_preprocessing.text.Tokenizer at 0x7fe876dddcc0>)"
            ]
          },
          "execution_count": 24,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# e.g.\n",
        "tokenize(['this place is good', \"こんにちは 今日は いい天気 。\", \"today is so cold\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KDfQw_5t9bA5"
      },
      "source": [
        "# create clean dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sIM8-10h9bA6"
      },
      "outputs": [],
      "source": [
        "# cleate a clean dataset\n",
        "def create_dataset(en, ja):\n",
        "    \n",
        "    # input_tensor, target_tensor: 2d numpy array\n",
        "    # input_lang_tokenize, target_lang_tokenize: word dictionary\n",
        "    input_tensor, input_lang_tokenize = tokenize(en)\n",
        "    target_tensor, target_lang_tokenize = tokenize(ja)\n",
        "\n",
        "    return input_tensor, target_tensor, input_lang_tokenize, target_lang_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jEJJ9BMgATdD"
      },
      "outputs": [],
      "source": [
        "input_tensor, target_tensor, input_lang_tokenize, target_lang_tokenize = create_dataset(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "lzlBqTlO9bA-"
      },
      "outputs": [],
      "source": [
        "def max_length(input_tensor, target_tensor):\n",
        "\n",
        "    # max length of input sentense and target sentense\n",
        "    english_len = [len(i) for i in input_tensor]\n",
        "\n",
        "    japanese_len = [len(i) for i in target_tensor]\n",
        "\n",
        "     # print max length\n",
        "    print(\"english length:\", max(english_len))\n",
        "    print(\"japanese length:\", max(japanese_len))\n",
        "    max_len_input =  max(english_len)\n",
        "    max_len_target =  max(japanese_len)\n",
        "\n",
        "    return max_len_input, max_len_target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "Ew-q24gJ9bBB",
        "outputId": "1939ab42-fd2c-463e-ff9a-27ec5356d451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "english length: 69\n",
            "japanese length: 86\n"
          ]
        }
      ],
      "source": [
        "# Calculate max_length of the target tensors\n",
        "max_length_input, max_length_target = max_length(input_tensor, target_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "Y_HvHj059bBG",
        "outputId": "29defb60-5136-49f5-b8f7-4fed80f5fe8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "240000 240000 30000 30000 30000 30000\n"
          ]
        }
      ],
      "source": [
        "# create trainnig set and validation set\n",
        "X_train, X_test, \\\n",
        "    Y_train, Y_test = train_test_split(input_tensor, target_tensor, test_size=0.2, shuffle=True)\n",
        "\n",
        "X_test, X_val, \\\n",
        "    Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, shuffle=True)\n",
        "\n",
        "\n",
        "# show length\n",
        "print(len(X_train), len(Y_train), len(X_test), len(Y_test), len(X_val), len(Y_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "w12Za_JK9bBJ"
      },
      "outputs": [],
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t != 0:\n",
        "            # Index number assigned to each word\n",
        "            print(\"%d----->%s\" % (t, lang.index_word[t]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "colab_type": "code",
        "id": "v6yT2odT9bBP",
        "outputId": "a0315d4d-13f0-4f54-9c22-e11d2642c993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input lang: index to word mapping\n",
            "1----->start_\n",
            "172----->call\n",
            "6----->the\n",
            "301----->car\n",
            "2----->_end\n",
            "output lang: index to word mapping\n",
            "1----->start_\n",
            "2040----->車を\n",
            "1551----->呼\n",
            "619----->べ\n",
            "2----->_end\n"
          ]
        }
      ],
      "source": [
        "print(\"input lang: index to word mapping\")\n",
        "convert(input_lang_tokenize, X_train[10])\n",
        "print(\"output lang: index to word mapping\")\n",
        "convert(target_lang_tokenize, Y_train[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dSHdoFwQ9bBT"
      },
      "source": [
        "# define parameter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "Jhj29n1l9bBT",
        "outputId": "ed8d71bc-0a88-4af1-c0b6-64bd45d2ecae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train step 7500\n",
            "Total unique words in the input: 52080\n",
            "Total unique words in the target: 25498\n"
          ]
        }
      ],
      "source": [
        "# BUFFER_SIZE >= dataset if smaller than dataset can't shuffle equally\n",
        "BUFFER_SIZE = len(X_train)\n",
        "BATCH_SIZE = 32\n",
        "dropout_rate = 0.3\n",
        "# if None steps_per_epoch == mum of dataset\n",
        "train_steps_per_epoch = len(X_train) // BATCH_SIZE\n",
        "val_steps_per_epoch = len(X_val) // BATCH_SIZE\n",
        "print(\"train step %d\" % train_steps_per_epoch)\n",
        "embedding_dim = 300\n",
        "units = 512\n",
        "vocab_inp_size = len(input_lang_tokenize.word_index) + 1\n",
        "print('Total unique words in the input: %s' % len(input_lang_tokenize.word_index))\n",
        "print('Total unique words in the target: %s' % len(target_lang_tokenize.word_index))\n",
        "vocab_tar_size = len(target_lang_tokenize.word_index) + 1\n",
        "\n",
        "# create train dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "# create validation dataset\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PjAVocfI9bCI"
      },
      "source": [
        "# Encoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "CJWSAHhl9bCJ"
      },
      "outputs": [],
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size, dropout_rate):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.enc_units = enc_units\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.first_lstm = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                                            return_sequences=True,\n",
        "                                                            recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "        self.final_lstm = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                                    return_sequences=True,\n",
        "                                                    return_state=True,\n",
        "                                                    recurrent_initializer='glorot_uniform')\n",
        "        \n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.first_lstm(x, initial_state =hidden)\n",
        "        output, state_h, state_c = self.final_lstm(x)\n",
        "        state = [state_h, state_c ]\n",
        "\n",
        "        return output, state\n",
        "        \n",
        "    def initialize_hidden_state(self):\n",
        "            return tf.zeros((self.batch_size , self.enc_units)), tf.zeros((self.batch_size , self.enc_units))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "V53dyDZO_F35"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZSn1VowJ9bCS"
      },
      "source": [
        "# attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "cctmI1mA9bCS"
      },
      "outputs": [],
      "source": [
        "class Attention(tf.keras.models.Model):\n",
        "\n",
        "    def __init__(self, units: int, *args, **kwargs):\n",
        "\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.units = units\n",
        "\n",
        "        self.q_dense_layer = Dense(units, use_bias=False, name='q_dense_layer')\n",
        "        self.k_dense_layer = Dense(units, use_bias=False, name='k_dense_layer')\n",
        "        self.v_dense_layer = Dense(units, use_bias=False, name='v_dense_layer')\n",
        "        self.output_dense_layer = Dense(units, use_bias=False, name='output_dense_layer')\n",
        "\n",
        "    def call(self, input, memory):\n",
        "\n",
        "        q = self.q_dense_layer(input) \n",
        "        k = self.k_dense_layer(memory) \n",
        "        v = self.v_dense_layer(memory)\n",
        "\n",
        "        depth = self.units // 2\n",
        "        q *= depth ** -0.5  # for scaled dot production\n",
        "\n",
        "        # caluclate relation between query and key\n",
        "        logit = tf.matmul(q, k, transpose_b=True) \n",
        "\n",
        "        attention_weight = tf.nn.softmax(logit)\n",
        "\n",
        "        attention_output = tf.matmul(attention_weight, v) \n",
        "        return self.output_dense_layer(attention_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QBBPzDxK9bCg"
      },
      "source": [
        "# Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "1nED5gCZ9bCh"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size, dropout_rate):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.dropout = Dropout(dropout_rate)\n",
        "        self.first_lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                                            return_sequences=True)\n",
        "        self.final_lstm = tf.keras.layers.LSTM(self.dec_units,\n",
        "                                                            return_sequences=True,\n",
        "                                                            return_state=True)\n",
        "                                                            \n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        self.attention = Attention(self.dec_units)\n",
        "    \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        x = self.embedding(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x =  self.first_lstm(x)\n",
        "        output, state_h, state_c = self.final_lstm(x)\n",
        "        state = [state_h, state_c]\n",
        "        attention_weights = self.attention(output, enc_output)\n",
        "        output = tf.concat([output, attention_weights], axis=-1)\n",
        "\n",
        "                \n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        output = self.fc(output)\n",
        "        \n",
        "        return  output, state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "37-lrVTN_F4X"
      },
      "outputs": [],
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fvyXf1bn9bC1"
      },
      "source": [
        "# optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wROZueWU9bC3"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.9, epsilon=1e-04, decay=1e-06)\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DtDLuqB49bC7"
      },
      "source": [
        "# Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "0CWEvq3W9bC8"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './train_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ja0mhQ4j9bDB"
      },
      "source": [
        "# train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TQSV9kkd9bDB"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([target_lang_tokenize.word_index['start_']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the sladecoder\n",
        "      predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return batch_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "colab_type": "code",
        "id": "rCI9q4-C9bDI",
        "outputId": "549c58c0-f50a-488e-c7ec-c9c8616709e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Train Loss 0.3737\n",
            "Epoch 1 Validation Loss 0.3688\n",
            "Epoch 2 Train Loss 0.3702\n",
            "Epoch 2 Validation Loss 0.3662\n",
            "Epoch 3 Train Loss 0.3672\n",
            "Epoch 3 Validation Loss 0.3622\n",
            "Epoch 4 Train Loss 0.3681\n",
            "Epoch 4 Validation Loss 0.3660\n",
            "Epoch 5 Train Loss 0.3711\n",
            "Epoch 5 Validation Loss 0.3687\n"
          ]
        }
      ],
      "source": [
        "# trained model for 45 epochs\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  train_loss = 0\n",
        "  val_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(train_dataset.take(train_steps_per_epoch)):\n",
        "    train_batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    train_loss += train_batch_loss\n",
        "\n",
        "\n",
        "  for (batch, (val_inp, val_tar)) in enumerate(val_dataset.take(val_steps_per_epoch)):\n",
        "    val_batch_loss = train_step(val_inp, val_tar, enc_hidden)\n",
        "    val_loss += val_batch_loss\n",
        "\n",
        "\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Train Loss {:.4f}'.format(epoch + 1,\n",
        "                                      train_loss / train_steps_per_epoch))\n",
        "  print('Epoch {} Validation Loss {:.4f}'.format(epoch + 1,\n",
        "                                      val_loss / val_steps_per_epoch))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hOhXwpZlDwJp"
      },
      "source": [
        "# Load trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "RVDzzIKJBsBU",
        "outputId": "4b01509c-c255-4c49-96ae-e38af7678bb1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fe795e7cda0>"
            ]
          },
          "execution_count": 89,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "checkpoint.restore(\"train_model/ckpt-5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "nQnK48R0BuZT"
      },
      "outputs": [],
      "source": [
        "# into base model\n",
        "encoder = checkpoint.encoder\n",
        "decoder = checkpoint.decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0gu5bPpdEHVz"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "Check bleu score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "wJ8r9JNREs01"
      },
      "outputs": [],
      "source": [
        "def predict(sentence):\n",
        "    inputs = tf.convert_to_tensor(sentence)\n",
        "    result = ''\n",
        "    inputs = tf.expand_dims(inputs, axis=0)\n",
        "    hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
        "    enc_out, state = encoder(inputs, hidden)\n",
        "    hidden_state = state\n",
        "    dec_input = tf.expand_dims([target_lang_tokenize.word_index['start_']], 0)\n",
        "    for t in range(max_length_target):\n",
        "        predictions, hidden_state = decoder(dec_input,\n",
        "                                                             hidden_state,\n",
        "                                                             enc_out)\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += target_lang_tokenize.index_word[predicted_id] + ' '\n",
        "        if target_lang_tokenize.index_word[predicted_id] == '_end' or len(result) > max_length_target:\n",
        "            return result\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "87pBzvXjRkJL"
      },
      "outputs": [],
      "source": [
        "def create_reference(lang, tensor):\n",
        "    all_sentence_list = []\n",
        "\n",
        "    for word_list in tensor:\n",
        "      sentence_list = []\n",
        "\n",
        "      for t in word_list:\n",
        "          if not t == 0:\n",
        "              # Index number assigned to each word\n",
        "              sentence_list.append(lang.index_word[t])\n",
        "      all_sentence_list.append(sentence_list)\n",
        "    return all_sentence_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JGwwlmHtRrfb"
      },
      "outputs": [],
      "source": [
        "# create reference\n",
        "reference = create_reference(target_lang_tokenize, Y_test.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "UAAMX-vQEKB_",
        "outputId": "ca4b91bb-c8f5-4a74-bc38-8722c206b50e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 30000/30000 [52:33<00:00,  9.51it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "# create hypothesis\n",
        "hypothesis = []\n",
        "for i in tqdm(X_test):\n",
        "  hypothesis.append(predict(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "-i0Jl_21Ee7k",
        "outputId": "21c06b14-9ad2-4c86-8c20-db42bfd5af9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The bleu score is: 0.12005058946101176\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "score = 0\n",
        "smoothie = SmoothingFunction().method2\n",
        "for i in range(len(reference)):\n",
        "    score += sentence_bleu([reference[i][1:-1]], hypothesis[i][:-5].strip().split(), smoothing_function=smoothie)\n",
        "\n",
        "score /= len(reference)\n",
        "print(\"The bleu score is: \"+str(score))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TNkXiemq9bDO"
      },
      "source": [
        "# Translation example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "J-DcisWA9bDP"
      },
      "outputs": [],
      "source": [
        "def preprocess_sentence(en_text):\n",
        "        # normalize English\n",
        "        en_text = en_text.lower()\n",
        "        en_text = english_unicode_to_ascii(en_text)\n",
        "        en_text = replace_special_character_to_space_en(en_text)\n",
        "\n",
        "        en_text = \"start_ \" + en_text + \" _end\"\n",
        "\n",
        "        return en_text\n",
        "        \n",
        "def evaluate(sentence):\n",
        "  \n",
        "    attention_plot = np.zeros((max_length_target, max_length_input))\n",
        "\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    inputs = [input_lang_tokenize.word_index[i] for i in sentence.split(' ')]\n",
        "\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                           maxlen=max_length_input,\n",
        "                                                           padding='post')\n",
        "    \n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    result = ''\n",
        "    hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
        "    enc_out, state = encoder(inputs, hidden)\n",
        "    hidden_state = state\n",
        "    dec_input = tf.expand_dims([target_lang_tokenize.word_index['start_']], 0)\n",
        "    for t in range(max_length_target):\n",
        "        predictions, hidden_state = decoder(dec_input,\n",
        "                                                             hidden_state,\n",
        "                                                             enc_out)\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += target_lang_tokenize.index_word[predicted_id] + ' '\n",
        "        if target_lang_tokenize.index_word[predicted_id] == '_end' or len(result) > max_length_target:\n",
        "            return result, sentence\n",
        "\n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "    return result, sentence, "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "Ui6bFxRt9bDd"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def result(sentence):\n",
        "    result, sentence = evaluate(sentence)\n",
        "\n",
        "    return result, sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "X2duMXKX9bDh",
        "outputId": "91e37647-4a87-4413-a5cb-c1835bab223d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: start_ he has a dog _end\n",
            "Predicted translation: 犬 が _end \n"
          ]
        }
      ],
      "source": [
        "result, sentence = result(\"he has a dog\")\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "XlCu-uvtwOft",
        "outputId": "5f49c5eb-a7bb-4ff4-f3fb-f880a27dfb9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: start_ what is this ! ? _end\n",
            "Predicted translation: これは 何 だよ な に _end \n"
          ]
        }
      ],
      "source": [
        "result, sentence = result(\"What is this!?\")\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "s2FZwaiky-yj",
        "outputId": "f4d68bbf-134a-43b7-e54f-f0fd9ff2c07a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: start_ actually i did it _end\n",
            "Predicted translation: や った _end \n"
          ]
        }
      ],
      "source": [
        "result, sentence = result(\"Actually I did it\")\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "67d09FoPz2Al",
        "outputId": "6fc6029c-d40f-4ef5-b3dc-6675c57aac1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input: start_ i love her _end\n",
            "Predicted translation: 彼女 を愛し て _end \n"
          ]
        }
      ],
      "source": [
        "result, sentence = result(\"I love her\")\n",
        "print('Input: %s' % (sentence))\n",
        "print('Predicted translation: {}'.format(result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "J2saSpKK0Cwl"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "translation_en_to_ja.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
