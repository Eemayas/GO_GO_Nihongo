{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fln_7HqX8Bi4",
    "outputId": "cff1cecb-b7db-4173-a981-5f5ab3898f52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lp2F4opx8ChB",
    "outputId": "480045c8-5ccb-4aa2-e597-39aa671b4aae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/.shortcut-targets-by-id/14kN-FziW24aUnE1LuiZ3m2W5UaK7G2vt/GO_GO_Nihongo\n"
     ]
    }
   ],
   "source": [
    "cd /content/drive/MyDrive/Projects/GO_GO_Nihongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqyjrsVR4PX-",
    "outputId": "dffd40f6-b917-4989-c9b6-472929e04811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mojimoji in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (0.0.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (0.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in e:\\7th sem\\go_go_nihongo\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install mojimoji\n",
    "%pip install sentencepiece\n",
    "%pip install nltk\n",
    "%pip install scikit-learn\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jK88JoQ79a_G",
    "outputId": "b7444e95-6eb3-4b7c-91a7-2788ba2f918c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n",
      "WARNING:tensorflow:From C:\\Users\\prash\\AppData\\Local\\Temp\\ipykernel_4848\\416642369.py:30: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, Input, dot, Activation, Concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import mojimoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "\n",
    "import nltk\n",
    "import unicodedata\n",
    "import sentencepiece as spm\n",
    "\n",
    "# ignore warning\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "print(tf.__version__)\n",
    "# gpu\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_L_ygdR9a_Q"
   },
   "source": [
    "# load text file\n",
    "\n",
    "this dataset is aleady implemented a SentenceSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jYlzMp1U9a_T"
   },
   "outputs": [],
   "source": [
    "num_example = 300000\n",
    "\n",
    "# create each languages list\n",
    "def create_lang_list(num_example):\n",
    "    # load txt file\n",
    "    lines =  io.open(\"datas/raw.txt\", encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "    word_pairs = [[w for w in l.split('\\t')]  for l in lines[:num_example]]\n",
    "\n",
    "    return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qG4dRId39a_X"
   },
   "outputs": [],
   "source": [
    "en, ja = create_lang_list(num_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrWjSfDlABKb"
   },
   "source": [
    "**SentencePiece**\n",
    "\n",
    "using pretrain model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "t5WysbRO9-nj"
   },
   "outputs": [],
   "source": [
    "ja_sentence = list()\n",
    "for i in ja:\n",
    "    ja_sentence.append(i.replace(\" \", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "J9M9slUl9_hh"
   },
   "outputs": [],
   "source": [
    "ja_text = list()\n",
    "# sp = spm.SentencePieceProcessor()\n",
    "sp =spm.SentencePieceProcessor(model_file='./Pretrained tokenizer/spm.en.nopretok.model')\n",
    "# sp.Load(\"wiki-ja.model\")\n",
    "for text in ja_sentence:\n",
    "    ja_text.append(\" \".join(sp.EncodeAsPieces(text)).replace(\"▁\", \"\").strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "laptNPxG8Vac",
    "outputId": "0f12ebd2-272f-45ed-bde6-2539545bbeb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['あ な た は 戻 っ た の ね ハ ロ ル ド ?',\n",
       " '俺 の 相 手 は シ ャ ー ク だ 。',\n",
       " '引 き 換 え だ あ る 事 と あ る 物 の',\n",
       " 'も う い い よ ご ち そ う さ ま う う ん',\n",
       " 'も う 会 社 に は 来 な い で く れ 電 話 も する な',\n",
       " 'き れ い だ 。',\n",
       " '連 れ て 行 け 殺 し そ う だ わ か っ た か ?',\n",
       " '殺 し た の か !',\n",
       " 'わ ぁ ~ ! い つ も す み ま せ ん 。 い い の よ ~ 。',\n",
       " 'カ ン パ ニ ー の 元 社 員 が',\n",
       " 'じ ゃ ア イ ツ ラ は ど こ ?',\n",
       " '相 手 を 陥 れ る と か ...',\n",
       " '必 要 の な い 子 耐 え る に して い る 。',\n",
       " '犯 人 像 と は 違 和 感 の あ る 見 た 目 だ が',\n",
       " 'あ !',\n",
       " 'も っ と 詳 し い の 見 せ ます から 。',\n",
       " 'そ れ 今 持 ち 出 す か ?',\n",
       " 'じ ゃ あ 仕 事 終 わ っ た ら 医 局 に 来 て よ 。',\n",
       " 'あ の ... 皆 さ ん で 先 済 ま せ て く だ さ い 。',\n",
       " '六 花 ち ゃ ん は ず っ と ず っ と ず っ と',\n",
       " 'ほ ら 別 に 変 な こ と じ ゃ な い で し ょ ?',\n",
       " '変 な 子 供 服',\n",
       " '船 長 から 他 の 誰 か が 扱 っ た 形 跡 が な い か d na を 再 調 査 する よ う に 頼 ま れ ま し た',\n",
       " 'も し 残 り も 上 手 く 書 け た ら',\n",
       " '以 前 に 増 して 魅 力 的 だ',\n",
       " '也 就 是 说 ... 该 不 会 是 ...',\n",
       " '長 い こ と 友 達 な の に 気 付 か な か っ た の か ?',\n",
       " 'ビ ル の ID 信 号 も 5 分 前 に 消 え た わ',\n",
       " '口 の 中 に 血 が 見 え る',\n",
       " '\" AS A MOTO は こ の ま ま じ ゃ い け な い と 思 う 。 \"',\n",
       " '嵐 に は 嵐 の',\n",
       " 'な る ほ ど 簡 単 に ゃ 入 れ そ う に ね え な',\n",
       " 'ニュー ス は ? 確 か に 二 人 の 危 険 に さ ら さ れ て',\n",
       " 'あ る 創 作 を し ます',\n",
       " '朝 、 電 話 する',\n",
       " '次 々 に 新 し い 柔 軟 な 勤 務 体 系 を 開 発 して い き ま し た',\n",
       " 'ル ーズ ベ ル ト ・ ア イ ラ ンド 橋 で',\n",
       " '僕 、 帰 る 所 が 無 い ん だ',\n",
       " '今 度 は わ し の 番 だ',\n",
       " 'ど う して 話 し た',\n",
       " '持 っ て い る だ け で 人 を 不 幸 に する 呪 わ れ た 本 な ど と い う も の は 存 在 し な い の で す',\n",
       " '悪 い と 思 う け ど 聞 か な き ゃ い け な い の',\n",
       " 'お 前 た ち を 英 雄 に し た り は し な い',\n",
       " '今 だ ! あ り っ た け の 封 印 札 を 投 げ つ け ろ !',\n",
       " 'そ の 結 果 犠 牲 を 払 う こ と に な り ます',\n",
       " '教 育 を 受 け',\n",
       " '何 か 悩 む ん が ア ホ ら し ゅ う な っ て 来 た わ 。',\n",
       " '俺 が 隠 して い る 魔 法 を か け る こ と も で き る',\n",
       " '帰 れ ! や だ ね 。',\n",
       " '裸 行 者 に 戻 り ます',\n",
       " '退 散 し ろ !',\n",
       " '初 回 に プ ロ フ ィ ル n 書 か さ れ ま せ ん で し た ?',\n",
       " '今 後 も こ の 状 況 は 続 く と 人 口 統 計 学 者 は 予 想 して い ます',\n",
       " '良 く や っ て く れ た',\n",
       " '逃 走 中 で す',\n",
       " 'ま さ か の 赤 ペ ン 先 生 !',\n",
       " 'う ... う ん',\n",
       " '蛭 間 病 院 長 に 聞 き な さ い !',\n",
       " 'ほ ん ま の 豪 傑 じ ゃ っ た ん じ ゃ ろ う ね ぇ 。',\n",
       " '倉 庫 っ て 。 あ っ 。 大 丈 夫 。 う ん 。',\n",
       " 'も う 一 度 現 れ た ら 顔 を 吹 っ 飛 ば す',\n",
       " 'コ ー ヒ ー と',\n",
       " '何 だ あ れ 。 あ の 屋 敷 。',\n",
       " '忘 れ ら れ た 町 ・・・',\n",
       " 'n yu 申 請 は 2 週 間 後 だ',\n",
       " 'エ ン ジ ン が 止 ま っ た',\n",
       " 'そ こ で 何 年 も 前 に わ た し は tree maker と い う プ ロ グ ラ ム を 書 き',\n",
       " '私 は こ の 58 人 の 患 者 の 話 が 聞 い て み た い ―',\n",
       " 'そ れ 以 上 に い い ぜ',\n",
       " 'は い え ~ ッ オ シ ャ レ ~',\n",
       " '僕 は そ の 2 つ を 僕 に 与 え ら れ な い 人 に 用 は な い 」',\n",
       " '彼 女 わ い は 彼 女 ば お る 身 で こ げ ん 所 挨 拶 に 来 よ っ と か',\n",
       " 'ま っ す ぐ 自 分 の 言 葉 は 曲 げ ね え 。',\n",
       " 'あ な た の 事 は 見 当 も つ か な い',\n",
       " 'あ っ 義 清 で は な い か 。 何 故 ?',\n",
       " '誰 で も い つ で も 何 で も 編 集 で き る よ う に し た ら',\n",
       " 'お 前 も',\n",
       " 'エ リ カ から だ っ た ら ? な ぜ 受 け 取 ら な か っ た ?',\n",
       " 'こ の 2 つ は 戦 争 の き っ か け で す',\n",
       " '導 い て い る と い う こ と で す',\n",
       " '良 い 人 だ っ た ら し い ね',\n",
       " 'エ マ 、 走 れ !',\n",
       " '死 亡 率 が 低 下 して い る',\n",
       " '私 は 何 を す れ ば ?',\n",
       " '彼 ら は 送 水 口 で 働 い て い る の 技術 者 を 守 っ て る',\n",
       " 'い や だ ~ ~ 歯 を 抜 く の は い や だ ~ ~',\n",
       " '山 の 神 々',\n",
       " 'そ れ を 最 も 強 く 具 現 化 し た の が こ の ゼ ー レ シ ュ ナ イ ダ ー だ 。',\n",
       " '象 で も と る の か ?',\n",
       " '4 カ 月 前 に は ま だ 出 来 な か っ た ん で す よ',\n",
       " 'え ッ ? あ の ... 漢 字 ド リ ル じ ゃ',\n",
       " 'し か し 私 は そ こ に い た 誰 一 人 同 様 の 危 機 に 対 処 し た',\n",
       " '車 に 行 く ん だ',\n",
       " '美 味 し い メ ロ ン の 育 て 方',\n",
       " '自 分 で ト ラ ン ク に 入 っ て く れ',\n",
       " 'ダ ブ リ ン 北 部 出 身 の ブ ラ ム ・ スト ー カ ー が 書 い た',\n",
       " '俺 も だ',\n",
       " 'ど う な っ た ? 川 に 飛 び 込 ん だ',\n",
       " '裸 は ど う や っ た ? 何 な ん す か そ れ は !',\n",
       " 'そ う は 思 わ な い わ',\n",
       " '多 分 両 親 の と こ ろ',\n",
       " '例 え ば ブ レ ン ト ・ ガ ー ヴ ィ ー に レ イ プ さ れ た 3 人 の 少 女 の 両 親 の よ う に ね',\n",
       " 'シ リ ア ル の 箱 の 裏 面 、 ボ ード ゲ ー ム な ど イン ク を 使 う あ ら ゆ る も の で',\n",
       " 'い い え で も ト ン プ ソ ン さ ん は 多 分 知 っ て る',\n",
       " 'ター ク 、 共 和 国 は ス パ イ ス を 持 っ て 来 た ?',\n",
       " '先 に 進 み た い ん で す',\n",
       " '国 内 の あ ー 人 々',\n",
       " '人 の 心 を 読 め な き ゃ 詐 欺 師 は 務 ま り ま せ ん から ね',\n",
       " '国 連 で の 15 年 間 は',\n",
       " '聞 い て な い の ?',\n",
       " '同 じ 意 見',\n",
       " 'ひ と り',\n",
       " 'は l 、',\n",
       " 'あ ぁ ぁ 分 か り ま し た',\n",
       " '姉 は そ の 人 と 一 緒 に な る べ き だ っ た と 思 っ て し ま う',\n",
       " '1500 ド ル の コ ン ピ ュ ー タ を 使 う こ と さ え で き れ ば 、',\n",
       " 'こ れ は 今 の う ち に ち ゃ ん と お 仕 置 き し と か ん と ... 。',\n",
       " '≪ ね ぇ ボ ウ ヤ ≫ え っ ... 。',\n",
       " 'オ レ た ち に は 五 感 を 超 え た 心 の 目 が あ る !',\n",
       " '船 外 モ ー ター を 使 っ て 別 の ボ ート で 岸 に 戻 っ た ん で し ょ う',\n",
       " 'ソ ナ タ で す 。',\n",
       " '今 日 一 番 の 驚 き だ な',\n",
       " '料 理 ? あ っ こ の 料 理 な ら 任 せ な さ い 。',\n",
       " 'そ れ を リ ア ル タ イ ム で ご 覧 に 入 れ ます が',\n",
       " '回 避 行 動 の 話 は 聞 い た か ?',\n",
       " '生 活 の す べ て が 葵 中 心 に ま わ っ て る 。',\n",
       " '行 く ぞ',\n",
       " 'ナ イ キ を 救 っ た の で す',\n",
       " 'd h : 準 備 は い い バ リー b f : 頭 の 上 だ な',\n",
       " 'ユ ー ジ ー ン は と て も 聡 明 な 男 だ っ た',\n",
       " 'ど こ で 見 つ け た ?',\n",
       " 'で も な ぜ 貴 方 と パ ー カ ー を 容 疑 者 に ?',\n",
       " '本 能 。',\n",
       " '何 と か な る',\n",
       " 'お し ゃ べ り',\n",
       " '美 知 香 今 そ ん な 話 して な い',\n",
       " 'は い 。 乗 っ た 。',\n",
       " 'お 願 い し ます',\n",
       " 'そ う い う 力 が あ る',\n",
       " 'n go で 働 く こ と に し ま し た',\n",
       " 'ま あ な',\n",
       " 'そ の 通 り こ こ から 歩 い て 出 て 行 く か',\n",
       " '私 も お 外 で 遊 び た い',\n",
       " '怒 り を 制 御 し た',\n",
       " '真 面 目 に や れ',\n",
       " 'ち わ 。 こ ん に ち は 。',\n",
       " 'ヘ ル メ ット を 着 け て も ら い ま し た',\n",
       " '素 晴 ら し い 人 で す',\n",
       " 'パ イ ロ ット だ',\n",
       " '進 行 で き ます か ?',\n",
       " '父 が 18 世 紀 の 時 計 職 人 の',\n",
       " '何 の た め に 太 刀 を 振 る っ て る の か ... 。',\n",
       " 'も う 1 度 チ ャ ン ス を く れ る と 思 う',\n",
       " 'こ れ は ... !',\n",
       " 'ビ ジ ター と 人 類 の',\n",
       " 'そ ん な 筈 は ...',\n",
       " 'く く っ ... 。 う ... う う ... 。',\n",
       " 'で も そ れ は で き な い',\n",
       " '城 の 中 の カ ラ ス を 殺 す の は 大 変 だ が',\n",
       " 'こ の 、 ノ ロ マ め !',\n",
       " 'う ~ ん',\n",
       " 'よ し い い',\n",
       " '一 体 ど う な っ て る ?',\n",
       " 'そ こ は 幸 せ な 場 所',\n",
       " 'や だ や だ や だ 。 や だ や だ 。 待 て ~ !',\n",
       " '全 く 違 う 雰 囲 気 で 奏 で る 事 に よ っ て',\n",
       " '何 ヶ 月 も の 画 策 や 行 動 が',\n",
       " 'リ タ',\n",
       " 'ど う に か 後 始 末 を し な き ゃ い け ま せ ん',\n",
       " '見 て イ サ ベ ル は 目 立 つ こ こ で 土 曜 日 に',\n",
       " 'そ れ を 自 分 が 黒 沢 っ て 男 に 頼 ん で',\n",
       " 'じ ゃ 解 い て',\n",
       " '代 表 の み な さ ん',\n",
       " 'リ ラ ック ス して く だ さ い',\n",
       " 'き っ と 戻 る っ て',\n",
       " '\" 1 . ア イ ・ コ ン タ ク ト \" \" 2 .',\n",
       " 'ペ ニ ス が あ っ た ら 私 な ら 試 す',\n",
       " 'こ こ は 墓 場 だ な',\n",
       " '言 っ た な !',\n",
       " 'そ れ は テ ロ の 生 産 者',\n",
       " '私 が 何 を す べ き か を 知 り ま せ ん で し た 。',\n",
       " 'は あ ? 何 が 負 け 惜 し み よ',\n",
       " '着 替 え する の だ っ て',\n",
       " 'こ い つ は ま い っ た ね 。',\n",
       " '悪 か っ た の は 全 部 こ っ ち な ん だ し 。',\n",
       " '何 て 事 だ !',\n",
       " '捕 ま え た よ 。 ほ ー ら 。 ウ ホ ホ ホ ホ ホ ホ ... 。',\n",
       " 'し か し パ ッ ド や ソ ル ボ セ イン の よ う な 素 晴 ら し い 衝 撃 吸 収 材 の お か げ で',\n",
       " 'ね え 先 生',\n",
       " '由 于 地 形 复 杂 自 然 环 境 恶 劣 人 迹 罕 至',\n",
       " 'ノ ック について な ん て 言 っ た か し ら ?',\n",
       " '真 里 谷 恭 介 。',\n",
       " 'そ れ から 美 由 紀 さ ん が 描 い た 似 顔 絵 も 。',\n",
       " 'お そ ら く 最 も 重 要 な',\n",
       " '先 へ す す む た め の',\n",
       " 'エ ー ジ ェ ン ト 46 な ん て 言 わ な い で ね',\n",
       " 'エ イ リ ア ン も 警 察 犬 を ? 知 ら な か っ た わ',\n",
       " '手 間 取 ら せ た り は し ま せ ん よ 。',\n",
       " '牢 獄 ね ... 。',\n",
       " '奴 を 逃 が す な',\n",
       " 'あ あ そ う ?',\n",
       " '許 して く れ 、 ム ッ シ ュ ー ・ グ ス タ フ 決 して 裏 切 る つ も り は 無 か っ た',\n",
       " '我 ら も 骨 折 っ て ご 奉 公 し た か い が あ っ た と い う も の に ご ざ い ます 。',\n",
       " '示 唆 して い ます',\n",
       " '何 か を 吹 き 飛 ば して 誰 か を 連 れ 出 す',\n",
       " '左 側 で は 10 万 も の ポ リ ゴ ン が 見 え ま し た',\n",
       " '部 屋 の あ ら ゆ る も の が 揺 れ て い ま し た',\n",
       " '何 だ ? あ の 音 は ?',\n",
       " '素 晴 ら し い 夜 だ 星 の 下 で',\n",
       " 'エ デ ン 第 49 ジ ャ ン ク ショ ン に 医 学 班 を',\n",
       " 'あ な た は も う わ か っ て る だ ろ 。',\n",
       " 'こ の 惑 星 で 起 こ る 様 々 な 物 事 の 原 因 と な っ て い ま し た',\n",
       " '. ご 主 人 に 電 話 を 頂 け れ ば',\n",
       " 'イ ンド に 対 する 過 大 評 価 で あ る',\n",
       " 'ク ソ',\n",
       " 'バ イ リ ン ガ ル は 認 知 能 力 に よ い 影 響 が あ る と い う ―',\n",
       " '俺 が い な き ゃ 宝 は 見 つ け ら れ な い',\n",
       " 'お い 三 代 目 様 だ っ て 大 変 な ん だ ぞ あ の お 歳 で 。',\n",
       " '本 当 に 一 生 懸 命 に な っ て ブ ラ ジ ル 中 で 売 ろ う と し ま し た',\n",
       " 'い つ も 退 屈 し な い な',\n",
       " 'web サイト を 特 定 して',\n",
       " '♪ み ん な で 歌 お う ぜ',\n",
       " '電 話 貸 せ 電 話',\n",
       " '頼 む ! オ レ を 鍛 え て く れ !',\n",
       " '特 殊 召 喚 !',\n",
       " '疲 れ た ま た 後 で ね',\n",
       " 'あ れ で 案 外 本 気 だ っ た ん だ よ 。',\n",
       " 'そ れ に 銀 行 は イ レ イン の 勤 務 先 だ',\n",
       " 'ク ル ール 戦 士 王 ク ル ール',\n",
       " 'ティ ム : 「 今 す ぐ に ! 」',\n",
       " 'で も 守 っ て や っ て ほ し い の',\n",
       " 'き れ い な 人 ね',\n",
       " 'お か ま い な く',\n",
       " '連 絡 し よ う と 思 っ て た ん で す け ど ね 。',\n",
       " '昨 日 の 今 日 で ま た 会 う と は ね 。',\n",
       " '空 中 で は 鳥 や 飛 行 機 の よ う に',\n",
       " '運 営 権 を 得 る こ と に し た ら ど う で し ょ う',\n",
       " '俺 は 姿 を 消 さ な い と',\n",
       " 'こ れ は 難 し い く な る ね ア ソ ー カ 静 に して',\n",
       " '爆 発 ま で 2 分 30 秒',\n",
       " 'フ レ ッ シ ュ ・ プ リ ン ス の よ う に 勝 手 気 ま ま',\n",
       " '姉 を ご 存 じ な ん で す か ? え え',\n",
       " 'う ま く い っ た ん で す か ? そ う な ん だ よ',\n",
       " 'ア ダ ム が 来 る',\n",
       " '胃 で ろ 過 して',\n",
       " 'こ れ が 問 題 だ と わ か っ て な お 偽 り の 対 策 で ご ま か さ れ て い る こ と で す',\n",
       " '俺 が 彼 女 の 世 話 を して る あ な た じ ゃ な く',\n",
       " 'そ な た が わ た し を し っ か と 認 識 で き る よ う に な る ま で だ 。',\n",
       " '火 薬 反 逆 罪 と 陰 謀',\n",
       " 'そ れ が ミ カ ゲ さ ん の 望 み な ら 私 は 叶 え て あ げ た い と 思 う',\n",
       " '大 切 な イ ベ ン ト を め ち ゃ く ち ゃ に し か け た の に な ぜ 何 も 言 わ な い ん だ ?',\n",
       " 'い い も の が あ る ぞ',\n",
       " 'そ れ が で き た な ら き っ と 俺 た ち は ... 仲 間 だ 。',\n",
       " 'そ う い う 自 分 が 想 像 で き な い ず ~ っ と 一 人 に 慣 れ て る から か な',\n",
       " 'よ ろ し く 伝 え と い て く だ さ い',\n",
       " 'ま ず は 海 や 淡 水 湖 に 住 む 藍 藻 類 で し た',\n",
       " '違 う 道 を 通 っ て ベ ガ ス に 行 く か こ っ ち で も 大 丈 夫',\n",
       " '話 に 矛 盾 が',\n",
       " '運 動 量 を 知 り た い で し ょ う し',\n",
       " 'た ぶ ん そ れ は な い',\n",
       " 'あ り が と ね ー',\n",
       " '顔 色 も い い で す し 。',\n",
       " 'あ の 計 画 は 完 成 して た ん だ',\n",
       " 'そ れ で 「 S . H . I .',\n",
       " '知 ら な い わ 、 本 当 に 知 ら な い の',\n",
       " 'チ ャ ック',\n",
       " 'で も カ プ ート に 頼 め ば か け さ せ て く れ る よ',\n",
       " '「 ど う な る か 覚 悟 し ろ よ 」',\n",
       " 'な ぜ 彼 が あ ん な マ ネ を ?',\n",
       " 'う わ あ ! あ っ ...',\n",
       " '32',\n",
       " 'こ こ に 住 ん で た 少 年 は ?',\n",
       " 'は い で き た',\n",
       " '待 っ て く れ',\n",
       " '俺 達 は 農 民 な の に 殺 し た',\n",
       " '間 違 い な い ん で す 。',\n",
       " '実 は こ の 写 真 は 去 年 の も の で す',\n",
       " '君 が ア ン ヌ ・ マ リー が や っ た 罪 を 終 わ ら せ て く れ',\n",
       " 'そ れ で も ゲ イ ラ は あ き ら め ず と う と う ―',\n",
       " '今 す ご く 大 事 な 会 議 中 で 動 け な い の',\n",
       " '1 時 間 未 満 で は 間 に 合 わ な い',\n",
       " '7 c fi 06 x よ',\n",
       " 'ど こ で 働 き ど こ で 遊 ん で い る か も',\n",
       " '俺 に 付 い て く る な よ',\n",
       " '親 子 二 代 に わ た っ て リ スト ラ さ れ ち ゃ っ て ん だ よ ね 。',\n",
       " 'ア ン ナ に は 知 ら せ な い 真 っ 先 に ア ン ナ に 届 け た ら ?',\n",
       " 'も う ! ハ ン カ チ も ... 。',\n",
       " 'し ... 失 礼',\n",
       " 'ア レ ク 不 明 な 指 紋 が あ る わ',\n",
       " '利 用 者 が 何 を 最 初 に',\n",
       " '完 璧 か ど う か は 我 々 が 決 め る こ と じ ゃ な い',\n",
       " 'そ こ から 百 万 倍 の 幸 福 を 得 る こ と は で き な い',\n",
       " '爆 弾 は ど こ だ ?',\n",
       " 'ダ ン ハ ム 捜 査 官 こ れ を',\n",
       " 'あ な た は パ ズ ル の 仕 組 み が わ か る の よ ね 私 達 を こ こ から 出 して',\n",
       " 'ど こ で 見 つ け た ?',\n",
       " 'あ り が と う ス モ ー キ ー',\n",
       " '小 さ な 魚 を 捕 ま え る た め に',\n",
       " 'で も そ れ で も',\n",
       " 'ティ ミ ー こ こ で 働 い て た ジ ャ ック は 知 っ て る か ? 彼 について 話 せ る 事 は ?',\n",
       " '受 け 入 れ な ん て 無 理 だ 。 そ れ が う ち の 診 察 券 を ➡',\n",
       " '調 子 に の る な よ ! 自 分 の ケ ガ は 自 分 の ミ ス だ !',\n",
       " '元 気 だ な お い ト ム !',\n",
       " 'あ れ は 私 の 息 子 の ボ ビ ー ・ ペ ナ で す',\n",
       " 'あ あ 、 そ の こ と は 聞 い た',\n",
       " '開 門 ! は ぁ ! ? 門 が 開 き は じ め た 。',\n",
       " 'こ の 研究 所 に 提 供 して る 資 金 は 役 に 立 っ て る わ',\n",
       " 'ほ か に 何 か 知 っ て る ?',\n",
       " 'そ れ は な い で す よ だ っ た ら 無 理 心 中 事 件 に',\n",
       " '男 女 間 に 違 い が あ る こ と が',\n",
       " 'こ こ が 心 臓',\n",
       " '患 者 の 情報 は 漏 ら せ な い の',\n",
       " 'シ ア ー シ ャ 、 伏 せ ろ 。',\n",
       " 'ゆ っ く り 前 へ 進 ん で い く .',\n",
       " 'ち ょ っ と 他 に 何 か な い の ? い い 方 法',\n",
       " 'そ して 私 た ち の 3 ヶ 月 の か わ い い 車 も',\n",
       " '僕 は レ ベ ル 2 の ア ー ミ ー モ ス キ ート ブ ラ ッ ド ・ フ ォ ー ス 3 体 で',\n",
       " 'こ の 状 況 は 不 利 だ わ',\n",
       " '年 齢 収 入 文 化 の 垣 根 を 越 え て 人 々 を 結 び つ け る',\n",
       " 'だ っ た ら 言 っ て い い ?',\n",
       " 'ど れ が 広 告 な の か 見 分 け に く い こ と も 多 い で す から',\n",
       " '家 を 共 有 して い る の に 近 い 感 じ',\n",
       " '「 な せ ば な る 」 っ て',\n",
       " '殺 して し ま う ぞ',\n",
       " '家 族 に 別 れ の メ ッ セ ー ジ を 残 し た い の',\n",
       " '怖 が ら せ て 立 ち 直 ら せ よ う と し た の',\n",
       " 'リ ス ペ ク ト は 理 解 して い る',\n",
       " 'フ ェ リ シ ティ ご め ん な さ い た だ ...',\n",
       " '灰 から 灰 に',\n",
       " 'な あ 怒 ら せ た な ら 謝 る',\n",
       " '私 た ち の 身 体 、 我 々 が 育 て る 微 生 物 製 品 そ して 建 築 物 ま で も が',\n",
       " 'こ れ が 俺 の 覚 悟 だ !',\n",
       " '病 気 の こ と は 分 か っ て る ク レ ア',\n",
       " 'ク リ ス ・ ホ ー キ ン ス が こ こ に 住 ん で ます ね',\n",
       " 'i po d に 対 する 8 ト ラ ック で あ り',\n",
       " 'い や ~ .',\n",
       " 'ビ デ オ ゲ ー ム だ よ',\n",
       " '皆 さ ん お め で と う ご ざ い ます 。',\n",
       " '「 『 イ ス ラ エ ル 』 っ て 検 索 して み て よ 」',\n",
       " 'ど う や っ て ?',\n",
       " '輸 血 を して る ん だ で も 専 門 の 医 師 に 診 せ た い',\n",
       " 'こ れ で あ い つ が ク ラ ス 代 表 か',\n",
       " '藍 染 に 聞 い て み な い こ と に は 。',\n",
       " 'こ の リ ス ク を 理 解 して い れ ば 民 主 主 義 制 度 を 守 る た め の 断 固 た る 措 置 を 取 れ ます',\n",
       " 'と に か く 週 末 出 勤 して で も 月 曜 ま で に 仕 事 を 片 付 け ろ',\n",
       " 'い ろ い ろ な 失 敗 や 苦 労 が あ っ た は ず で す 。',\n",
       " '既 に 4 人 会 っ た わ',\n",
       " 'あ あ う ま い な 氷 を 入 れ て く れ な い か ?',\n",
       " '手 出 して く だ さ い 。',\n",
       " '子 ど も の 頃 の 誕 生 日 会 を 覚 え て る だ ろ',\n",
       " 'さ あ 2 人 と も 行 進 よ',\n",
       " 'で で き な か っ た',\n",
       " '私 た ち も 科 学 も ほ と ん ど 知 ら な い こ と で す',\n",
       " '覚 え て る よ ぐ ら い 言 い な さ い よ',\n",
       " 'ヘ ン リー 、 行 く の よ !',\n",
       " '喜 ん で',\n",
       " 'こ れ は 帰 り 道 で',\n",
       " '中 年 ま で 老 年 性 の 病 気 に な ら な い こ と から 明 ら か な の で す',\n",
       " '今 助 け に 行 く',\n",
       " 'お 酒 よ',\n",
       " '皆 昇 進 を 手 に で き ず 心 底 怒 っ た の で す',\n",
       " '\" ハ ード ワ ー ク は い ハ ード プ レ イ は い ? 。 ?',\n",
       " 'お 腹 一 杯 だ こ こ に 来 る 途 中 に 食 べ て き た',\n",
       " '家 じ ゅ う に あ る',\n",
       " 'カ リ を あ な た の テ ー ブ ル に 呼 ん じ ゃ い ま し ょ う',\n",
       " '知 っ て し ま っ た ら 辛 く な る から ...',\n",
       " 'え え お ま ん こ な め て',\n",
       " '私 を 破 壊 し た の だ 君 が',\n",
       " '居 る べ き じ ゃ な い ...',\n",
       " 'そ こ の 住 所 を 送 る わ',\n",
       " '最 初 の 2 文 字',\n",
       " '敵 の 弱 点 を 調 べ て い る 間 に 自 分 自 身 の こ と を 知 る こ と も 重 要 だ',\n",
       " 'だ め 、 カ イ ル !',\n",
       " '語 り 継 ぐ べ き 人 生 の 物 語 を 紡 い だ の で す',\n",
       " '右 足 だ と ど う な ん だ い ?',\n",
       " 'い や ぁ ま さ か あ ん た に ➡',\n",
       " 'う ま い こ れ マ ジ で う ま い 。 い い ぞ ヒ メ コ 殿 。',\n",
       " '女 性 と 子 ど も た ち が',\n",
       " '御 坂 さ ん お 話 っ て 何 で し た',\n",
       " '知 り た い 理 由 な ど 無 い',\n",
       " 'あ い つ 色 々 話 して く れ る っ て よ 。',\n",
       " '時 間 を 割 い て 頂 き あ り が と う ご ざ い ます ヒ ル デ ・ エ ン ゲ ル について で す',\n",
       " '愛 は 重 要 じ ゃ な い と ― そ ん な フ リ を して る が い つ も 考 え て る',\n",
       " 'ぎ ぃ っ ! あ っ ! あ あ !',\n",
       " 'だ から 兄 貴 と して は 弟 の 邪 魔 は で き な い っ て い う か',\n",
       " 'ど こ に 行 っ た ら い い ?',\n",
       " '素 晴 ら し い',\n",
       " 'お 前 の 見 つ け た 信 管 から 微 量 だ が 血 液 反 応 が 出 た',\n",
       " '忘 れ ち ま っ て る ん だ',\n",
       " 'ガ セ か も み ん な で 見 た 気 に ...',\n",
       " 'で す が 収 穫 は あ り ま し た',\n",
       " 'じ ゃ あ オ リー ブ に 伝 え て 捕 ま ら な い っ て',\n",
       " 'い く ら 押 収 し た ?',\n",
       " 'し か し 無 謀 さ は 過 去 の 歴 史 に 何 度 も あ る こ と で す',\n",
       " 'そ う い う 訳 で 本 当 に ご め ん な さ い と 彼 に 伝 え た い の',\n",
       " 'そ の 前 に 清 家 が 説 得 に 向 か っ た',\n",
       " 'マ ン 、 私 た ち は こ の た わ ご と の た め に 古 す ぎ を 得 て い る 。',\n",
       " 'た く さ ん の 大学 の 教 授 から の',\n",
       " '電 話 し な い で ね 、 こ っ ち から 連 絡 する',\n",
       " '何 が 選 択 だ ?',\n",
       " '偶 然 を 装 っ て 何 回 か 地 味 に 蹴 ら れ ま し た 。',\n",
       " 'じ ゃ あ ... ゴ フ ッ ! 早 く 行 け 。 し か し ... 。',\n",
       " 'な に っ ! ? 「 ク ズ 鉄 の か か し 」 !',\n",
       " 'ツ チ 系 難 民 の 捕 虜 収 容 所',\n",
       " 'で も そ れ は 無 く な っ た',\n",
       " '地 球 に も た ら さ れ た 。',\n",
       " '部 外 者 は 決 して 来 ら れ な い 君 が 初 め て だ',\n",
       " 'そ ん な こ と 気 に する な っ て',\n",
       " '同 じ だ ね',\n",
       " 'ル イ ス こ こ で 何 を ?',\n",
       " 'な ぜ 証 人 が 必 要 な ん だ ?',\n",
       " '狂 っ て な い 君 の た め に や っ た',\n",
       " '忙 し か っ た から こ こ で 1 泊 し た の',\n",
       " 'で は 異 な っ た 事 業 者 を',\n",
       " '私 の 味 方 じ ゃ な い の か ? 聞 い た 事 を 告 げ て る の',\n",
       " '尾 行 から 逃 げ る の は 事 態 を 悪 化 さ せ る だ け だ',\n",
       " '《 シ カ マ ル : サ ク ラ ど こ ろ じ ゃ ね え 。',\n",
       " 'イ ー ビ ー を 見 ら れ た !',\n",
       " 'す ぐ に わ か る',\n",
       " 'そ う な ん だ',\n",
       " '特 定 の フ ライ ト や 座 席 に リ ン ク する と い う こ と で す',\n",
       " 'そ れ は つ ま り ?',\n",
       " '思 い 出 す ん で す',\n",
       " '酷 い 日 焼 け',\n",
       " '元 気 出 して',\n",
       " 'ワ サ ジ ー 郡 の 一 部 上 空 で は',\n",
       " '等 我 弄 清 楚 了 再 告 诉 你',\n",
       " 'あ の 方 向 に 住 む 民 から の 知 ら せ を 持 っ て き た',\n",
       " '3 年 前 の 手 術 で 頭 部 を 傷 つ け て 早 期 発 症 の 引 き 金 に な っ て い た か も',\n",
       " 'ど の く ら い 続 き そ して 何 を 意 味 する の で し ょ う か ?',\n",
       " '分 か っ た',\n",
       " '困 惑 して い る の は 分 る',\n",
       " 'そ れ が あ い つ と 子 供 の た め に な る っ て ➡',\n",
       " '息 子 ま で 出 来 て 幸 せ だ わ',\n",
       " 'あ の ユ ン ボ ロ イ ド は 高 い 技術 の 産 物 だ と 感 じ ます',\n",
       " 'で そ の 患 者 ら は 何 を して た っ て ん だ',\n",
       " '人 間 の 手 に よ る 災 禍 で あ る よ う に',\n",
       " 'お わ か り で し ょ う こ の 人 物 は 具 体 的 な 世 界 と い う ―',\n",
       " '誇 張 さ れ 過 大 評 価 さ れ て い た ら ? 実 は 緊 急 性 も な く',\n",
       " 'な に シ ン ク ロ して ん だ',\n",
       " '昼 飯 見 つ け た っ て ば よ 。',\n",
       " '是 非 あ な た に 会 い た い っ て',\n",
       " '知 っ て た ら い い ん だ け ど そ し た ら',\n",
       " \"そ れ は ク ダ い く つ 身 代 金 の た め に 見 て い る だ け 小 さ な 部 分 ' き 。\",\n",
       " 'で も 駆 除 だ け は さ して も ら い ます よ',\n",
       " '「 イン ター ネ ット に お け る 経 済 的 創 造 性 へ の 現 実 の 脅 威 な ら び に 知 的 財 産 窃 盗 の 予 防 」',\n",
       " 'わ し ら の 方 こ そ 姫 、 こ ち ら こ そ だ',\n",
       " '卒 業 が 全 然 決 ま っ て な い や つ の 口 から',\n",
       " 'そ う だ な ま ず は ...',\n",
       " '最 後 に は み ん な あ ん た の こ と を 心 配 して た !',\n",
       " '森 と の 関 連 が な い か を 調 べ て い ます',\n",
       " '我 々 の 心 を 動 か して い る わ け で す',\n",
       " '楽 し か っ た よ 青 子 。',\n",
       " '私 は 家 の 中 に 入 っ て 階 段 を 上 り',\n",
       " 'ふ ざ け ん な こ の 野 郎 !',\n",
       " '俺 た ち は お 前 が 必 要 だ',\n",
       " 'チ ー ム 3 が 上 の 階 に 向 か っ た',\n",
       " '視 覚 障 害 者 が 自 ら 判 断 し 運 転 で き る 車 だ っ た の で す',\n",
       " 'も し か して そ れ っ て 私 を フ ォ ロ ー し よ う と して ん の ?',\n",
       " '教 師 に な る 前 に',\n",
       " 'あ あ あ あ 大 丈 夫 だ 坊 や 泣 く な',\n",
       " '君 が 気 に 入 る 面 白 い 話 を し に 来 た ん だ',\n",
       " 'と 言 っ た も の の こ の 世 に 1 人 だ け',\n",
       " 'チ ェ ック を し な い と 忘 れ ら れ た り 見 逃 さ れ る',\n",
       " 'い つ 頃 こ っ ち に 着 き そ う ?',\n",
       " 'わ か る わ 。 私 の お 尻 も そ う で す から 。',\n",
       " '他 に も 容 疑 者 を 探 す な ら 指 紋 照 合 を して み れ ば ?',\n",
       " 'な ぜ か 8 年 前 よ り も',\n",
       " 'も ち ろ ん 忘 れ な い て な い わ な ん て 驚 き ...',\n",
       " '戻 して',\n",
       " '全 然 マ ジ な ん か じ ゃ な い',\n",
       " 'あ な た が 育 て た 子 が 死 ん だ の ?',\n",
       " 'そ ん な 恐 ろ し さ を 私 は よ く 知 っ て い ます',\n",
       " 'モ ル ディ ブ の 風 で 私 が ク ール に な る よ う に 祈 っ て な',\n",
       " '私 名 前 は メ チ ャ ・ モ チ マ ッ ヅ ィ と 申 し ます',\n",
       " 'ダ ニ ー 候 補 が ター ボ と 関 係 が あ っ て エ リ オ ット 撃 つ よ う 仕 向 け た と か',\n",
       " '最 初 の も の は 転 送 完 了 ま で 1 分 半 も か か っ た',\n",
       " '菌 根 は 4 億 5000 年 前 から 存 在 し',\n",
       " '誰 か 来 る !',\n",
       " '私 の 問 題 だ と 思 う から 。',\n",
       " '同 じ 船 で 生 活 して て 噂 一 つ 耳 に 入 ら な か っ た と ?',\n",
       " '変 わ ら ず こ こ に い る',\n",
       " 'い つ で も 真 面 目 で 一 生 懸 命 な の も ポ イン ト 高 い け ど',\n",
       " 'も う 一 度 そ れ を で き る か 知 り た い ん だ',\n",
       " 'あ り が と う',\n",
       " '当 然 理 由 は 分 か っ て い ます ね',\n",
       " '何 して る ん だ ? - 投 資 だ よ',\n",
       " '銃 を 持 っ た 人 種 差 別 主 義 者 と 評 して',\n",
       " '火 は 風 を 受 け る と ➡',\n",
       " 'ち ょ っ と 話 して も い い ?',\n",
       " '遺 伝 子 を 研究 して',\n",
       " 'ト ル ティ ー ヤ ・ チ ップ ス を 頼 ん で る の ?',\n",
       " 'つ い に 関 節 が 動 か な く な る こ と も あ る の だ',\n",
       " 'な ん か し ま し ょ う か',\n",
       " '私 は 彼 の 目 を 矢 で 貫 く 聖 ウ ォ ー カ ー へ よ う こ そ 市 会 議 員',\n",
       " '4 年 間 世 界 中 を 旅 して',\n",
       " '乗 務 員 を 募 集 して い る ら し い .',\n",
       " 'さ あ',\n",
       " '違 う 内 容 に な り ます',\n",
       " '金 木 君 去 探 望 下 她 吧',\n",
       " 'ど こ ま で 逃 げ て も 過 去 は 追 っ て く る',\n",
       " 'ヴ ィ ッ キ ー · ネ ル ソ ン と い い ます',\n",
       " '思 い 出 せ 一 護 !',\n",
       " '真 実 か ... 重 す ぎ る',\n",
       " 'ど う よ え ? そ ん な テ ー マ パ ー ク 聞 い た こ と な い し 新 し い や ろ ?',\n",
       " '話 し が あ る ん だ',\n",
       " '野 菜 ベ ー コ ン だ',\n",
       " '郷 田 さ ん が 死 ん だ と き',\n",
       " 'こ れ は キ ャ バ ク ラ 通 い が',\n",
       " 'こ り ゃ ど ー も ー',\n",
       " 'ナ イ ジ ェ ル ど け よ',\n",
       " 'ど う 使 う の ?',\n",
       " 'エ ク エ スト リ ア で は 何 で も 魔 法 を 使 っ て して た ん で し ょ ?',\n",
       " 'そ れ が 君 達 の d na な の だ',\n",
       " 'わ か っ た じ ゃ',\n",
       " '我 比 较 喜 欢 你 叫 我 数 据 库',\n",
       " 'あ な た の 仕 事 を す ご く 誇 り に 思 う わ',\n",
       " 'そ の す べ て が 任 務 だ っ た 。',\n",
       " '気 候 変 動 に は 様 々 な 自 然 要 因 が 関 係 して い ます',\n",
       " '俺 の 歯 が 立 た な い シ ス テ ム な ら 彼 ら の お 客 に と っ て も 十 分 安 全 だ と 言 え る だ ろ',\n",
       " 'も う 一 度 澪 に 恋 を し ま し た',\n",
       " '秩 序 と 平 穏 を 取 り 戻 そ う と し ます も し',\n",
       " '背 の 高 い 奴 は 、 気 味 が 悪 い',\n",
       " '江 利 子 さ ん と 関 係 が あ る ん で す ね ?',\n",
       " '折 角 の 新 婚 旅 行 な の に',\n",
       " '... ベ ル ト だ し',\n",
       " '違 う 。',\n",
       " 'す み ま せ ん 大 統 領 彼 女 が む り や り',\n",
       " 'ト ラ ン ザ ク ショ ン の 大 半 は そ う で す',\n",
       " 'そ の 範 囲 内 に お い て',\n",
       " 'も し ア バ ド ン が 勝 て ば お 前 は 全 て に サ ヨ ナ ラ の キ ス だ ぞ 派 手 な 車 に も 本 の 取 引 も',\n",
       " 'ア メ リ カ ...',\n",
       " '婚 約 パ ー ティ の ?',\n",
       " 'そ れ から ア ル マ に 起 き た',\n",
       " 'そ の た め に oc r',\n",
       " '助 け て く れ 何 と か な ら ん の か 他 に 救 急 車 は ?',\n",
       " '情報 を 操 作 する',\n",
       " 'パ ー カ ー 、 2 、 3 ヵ 月 激 務 !',\n",
       " '市 場 経 済 ル ール 下 で の 労 働 の 機 会 を',\n",
       " '何 % が 海 外 へ の 直 接 投 資 と して',\n",
       " '「 こ う し た 動 き の 中 で ヤ ヌ ス ヴ ォ リ チ ェ ク 枢 機 卿 が 」 法 王 庁 で の 議 論 を 主 導 「 今 の と こ ろ 公 式 な 回 答 は 無 く 」',\n",
       " 'よ して く れ 。 俺 は 酒 も 性 癖 も スト レ ート だ 。',\n",
       " '金 は ど こ に あ る ん だ ?',\n",
       " 'あ い つ が 言 っ て た',\n",
       " 'モ ー リ ン ?',\n",
       " 'ボ ク は ... 何 か を 取 り 戻 そ う と して た 何 も な か っ た',\n",
       " 'す ご い 放 射 線 量',\n",
       " 'ケ ル ティ ト レ ン ト ・ ケ ル ティ で す 私 た ち は お 隣 同 士 で',\n",
       " 'そ う ど ん な 種 類 の ?',\n",
       " '悪 い け ど 俺 負 け た こ と な い ん だ よ ね 。',\n",
       " '黒 子 ひ ょ っ と して い ら な い 子',\n",
       " '当 選 よ',\n",
       " 'あ い つ の 仕 業 だ',\n",
       " '正 直 矛 盾 も 感 じ ま し た 。',\n",
       " '弁 護 士 と い う',\n",
       " '提 供 する の は あ な た の 義 務 で も あ る の よ 。',\n",
       " '謝 罪 の 代 わ り に 何 で 真 実 を 言 わ な い の !',\n",
       " '俺 は 力 が ... 強 さ が 欲 し か っ た 。',\n",
       " 'コ ン ピ ュ ー ター の イン ター フ ェ ー ス デ ザ イン で は',\n",
       " 'き っ と 僕 が 求 め て い た 所 だ よ ね',\n",
       " 'ど こ を 回 っ た ?',\n",
       " 'こ の よ う な 特 別 な 規 模 で の イ ノ ベ ー ショ ン が 起 こ り 得 る の で す',\n",
       " 'し か し 現 実 に は',\n",
       " '非 常 に 速 く 商 品 化 が 進 ん で い ます',\n",
       " '皮 膚 が ...',\n",
       " '20 の 質 問 で す か い や 120 の 質 問 か も し れ ま せ ん ね',\n",
       " 'こ ん な に か わ い ら し く な い',\n",
       " 'あ と 1 回',\n",
       " 'や っ た ー ! 当 然 当 然 。',\n",
       " '僕 の ゲ ー ム に 乗 っ て も 乗 ら な く て も',\n",
       " 'ど う か 私 た ち を 見 逃 して く だ さ い ... と ね 。',\n",
       " 'レ イ ・ デ ル タ 、 聞 こ え る ?',\n",
       " '台 北 の く そ っ た れ っ !',\n",
       " '瞳 ち ゃ ん 加 々 見 君 こ っ ち 来 て 。',\n",
       " 'よ う こ そ ジ ュ ラ ッ シ ック ・ ワ ール ド へ',\n",
       " 'そ れ で は 続 け ます',\n",
       " 'ネ コ ?',\n",
       " 'お い 幸 村 一 生 懸 命 す ぎ て 聞 こ え て な い ね',\n",
       " '嫌 に な る わ',\n",
       " '理 解 で き ます',\n",
       " '鑑 識 が な に か わ か っ た か 聞 い て き ます 。',\n",
       " 'そ して も う 一 度 僕 た ち の ゲ ー セ ン を 始 め る ん だ 。',\n",
       " 'そ の 無 線 で 俺 に 知 ら せ ろ',\n",
       " '今 日 の と こ ろ は こ の く ら い に し と け 。',\n",
       " '折 木 同 学',\n",
       " 'そ して 耳 元 で 囁 く \" 今 ど ん な 気 分 ?\"',\n",
       " '斧 を 消 毒 して ・ ・',\n",
       " '真 偽 を 確 か め る の は 簡 単 で す',\n",
       " '彼 女 は 腰 に 手 を 当 て',\n",
       " 'お い し そ う な 香 り が シ ート 越 し に 漂 っ て き た ら',\n",
       " 'え え 、 か ま い ま せ ん よ',\n",
       " '俺 は 地 上 で 容 疑 者 を 追 う お 前 は 交 渉 に 専 念 し ろ',\n",
       " 'わ ... わ し で す か ?',\n",
       " '助 け て ん',\n",
       " 'ま か り 通 っ て い る こ と で す',\n",
       " '20 年 間 西 洋 で 学 ん で き た こ と を 話 す 気 分 で す',\n",
       " '来 い R 2 レ イ ア 姫 を 探 そ う',\n",
       " '夢 を 見 て い た だ と ?',\n",
       " 'そ う ト ゲ が あ る ん だ た く さ ん あ る で し ょ う ? 痛 い よ ね',\n",
       " '交 通 局 の デ ー タ を 見 る',\n",
       " '船 と 運 命 を 共 に する の は 船 長 だ け だ ろ',\n",
       " 'そ れ で も 男 か ?',\n",
       " '圧 倒 的 に 押 し 広 げ',\n",
       " 'こ れ は た っ た 数 年 前 に 発 見 さ れ た も の で',\n",
       " 'ま 頑 張 っ て よ 森 夫 ち ゃ ん 。',\n",
       " '命 を 奪 う こ と に ... 夢 中 に っ て い た 、 だ が 僕 は 、 ...',\n",
       " 'あ な た 達 二 人 は 立 派 な 協 力 者 に な る と 思 い ます',\n",
       " '役 割 が あ る ん だ',\n",
       " 'た だ の ビ ッ チ よ 私 達 に 関 係 な い',\n",
       " '2 組 の 魔 女 っ 子 ち ゃ ん',\n",
       " 'プ ロ パ ン は 全 部 や る よ',\n",
       " 'あ の 私 の 手 柄 に 対 する ご 褒 美 な ど は ...',\n",
       " 'タ ブ レ ット の 電 源 を 切 る の',\n",
       " '報 酬 は 現 金 で 欲 し い',\n",
       " 'フ ライ ト キ ー を 手 に 入 れ た',\n",
       " 'さ て 、 あ な た の 後 ろ に 軍 隊 で 、 あ な た は 非 常 に 政 治 的 で あ る 可 能 性 が あ り ます 。',\n",
       " '油 は き っ か け を つ か む た め の も の じ ゃ と 。',\n",
       " 'あ ん た の 友 達 か い ?',\n",
       " '恋 に 落 ち ― え え 、 彼 を 愛 して い る わ !',\n",
       " '分 か り た く も な い よ !',\n",
       " '違 う 文 明 の 違 う シ ナ リ オ も あ り 得 る で し ょ う',\n",
       " '何 か 方 法 は 無 い か ?',\n",
       " '寂 し か っ た ん で す よ',\n",
       " '人 工 知 能 型 os 開 発 や',\n",
       " 'ね ぇ パ ト リ ック あ な た っ て 子 供 っ ぽ い 希 望 を 持 っ て い る の ね 本 当 に す ご く 可 愛 い わ',\n",
       " '過 去 の ど の あ な た と も 同 じ よ う に',\n",
       " '何 と か して ... 心 が 動 じ な い よ う 弁 解 を 考 え る ...',\n",
       " 'Mo は 枦诵 販 売 して お り ます 。',\n",
       " '理 由 は 分 か っ て る だ ろ',\n",
       " 'こ れ よ り 萌 え 萌 え じ ゃ ん け ん 大 会 総 選 挙',\n",
       " '間 違 い な く あ な た の せ い な の よ',\n",
       " '但 怎 样 都 降 不 下 来',\n",
       " 'ま た ま た 。 冗 談 き つ い で す よ 。',\n",
       " 'デ ザ イ ナ ー は 形 式 と 内 容 の 関 係 に 注 目 し ます',\n",
       " '逃 げ ろ ク ロ ッ !',\n",
       " 'こ の 弁 護 士 について 何 か 判 ら な い か ?',\n",
       " '中 の 魔 女 が 出 て く る ま で に は ま だ 時 間 が あ る け ど',\n",
       " '私 達 が ト マ ス が 何 を して る か 分 か る 前 に 動 く な ら',\n",
       " '今 日 ア メ リ カ 人 の 3 5% が',\n",
       " 'も う 一 度 ト イ レ だ',\n",
       " '\" レ ベ ル c \"',\n",
       " '疲 れ 切 っ て い た の か',\n",
       " '興 味 深 く な い ?',\n",
       " '「 死 に た い 」 や 「 自 殺 し た い 」 と メ ッ セ ー ジ を 送 れ ば',\n",
       " '涙 な ん か 流 し ち ゃ っ て あ ほ ら し か っ た',\n",
       " '明 日 間 違 い な く ゴ ル ゴ は そ こ に 現 れ る',\n",
       " '「 誰 が 何 を し た の か 」 を で す',\n",
       " '足 の 裂 傷 だ が 他 に ケ ガ は 無 い',\n",
       " 'な ん て こ っ た',\n",
       " '脳 切 開 は 任 せ て く れ',\n",
       " '唯 一 の 繋 が り は あ な た',\n",
       " '幽 霊 会 社 から 押 収 し た 書 類 に',\n",
       " '沈 黙 が 誰 か の た め に な る と ?',\n",
       " 'ど う す べ き だ っ た ん だ ろ う',\n",
       " 'ウ ォ ー レ ン 、 邪 魔 は する な',\n",
       " '「 浮 遊 島 」',\n",
       " 'サ イ モ ン ・ テ イ ラ ー - - あ な た は 今 日 彼 ら の 人 生 を 滅 ぼ し た',\n",
       " '外 に い る の は 私 の 信 頼 する 部 下 だ お 前 の た く ら み を 知 っ た と き ...',\n",
       " '完 全 に フ ィ ット し ます',\n",
       " 'あ え て 突 っ 込 ん で い っ た ん だ 。',\n",
       " '良 し ニ コ ラ ス 2 番 目 の 質 問',\n",
       " '計 算 が あ う の か な な ん て ア ハ ハ ハ ハ 。',\n",
       " 'そ の 理 由 は 古 い か も し れ ま せ ん が',\n",
       " '高 い 部 屋 を と っ た ん だ',\n",
       " 'じ ゃ あ ど う す り ゃ い い ん だ よ ?',\n",
       " '使 っ た 記 憶 が ご ざ ん せ ん 。 ど ん な 呪 文 な ん で す か ?',\n",
       " 'こ れ は ボ スト ン ダ イ ナ ミ ック の B ig Do g で す',\n",
       " '写 真 を 指 差 して',\n",
       " '結 婚 の 弱 点 を 試 し た い ?',\n",
       " 'ジ ェ ット ファ イ ヤ ー が 言 っ て た 亡 骸 ?',\n",
       " '俺 の 兄 弟 は ジ ェ ー ム ス と 街 に い る は ず だ よ',\n",
       " '言 う 通 り に して く だ さ い',\n",
       " '生 ま れ た と して も そ れ が 劣 っ た 行 動 に',\n",
       " '逃 げ 延 び た も の 達 から 話 を 聞 い た',\n",
       " 'な ご ま せ た く て',\n",
       " 'xxx の in sec to を 削 減 する 方 法 。',\n",
       " '別 に 恥 ず か し く な い さ',\n",
       " '総 員 戦 闘 配 備 !',\n",
       " '私 は 、 お 粗 末 な 料 理 だ 。',\n",
       " 'だ が 断 る',\n",
       " '隣 り 合 っ て 座 る と き は 別 で す が',\n",
       " 'も う す で に 出 来 上 が っ て い た 気 が し ます',\n",
       " '今 向 か っ て る',\n",
       " 'や る な ぼ ー や',\n",
       " '道 路 や 電 話 回 線 な ど の イン フ ラ が 無 い た め で は な く',\n",
       " 'チ ャ ー リー は 死 ん だ',\n",
       " '運 命 の 時 が !',\n",
       " '私 は 「 教 え て い ます よ',\n",
       " 'ラ ン ナ ー は 一 般 人 の も よ う で す 。',\n",
       " 'ジ ュ ン ピ ョ の 居 場 所 を 教 え て く だ さ い',\n",
       " '公 衆 衛 生 部 門 だ っ た',\n",
       " '人 は 体 を 折 り 曲 げ て 自 分 の 箱 に 入 る ん だ',\n",
       " 'そ の 泥 を 分 光 計 に か け る と',\n",
       " '精 神 異 常 犯 罪 者 の た め の ...',\n",
       " 'こ の 電 話 じ ゃ 無 理 だ が ター ゲ ット に は ル ー ター が あ っ た',\n",
       " 'ゾ ン ビ 映 画 っ て 似 た よ う な 名 前 ば っ か だ な あ',\n",
       " '今 は 、 こ っ ち が 先 よ',\n",
       " 'そ こ を 強 調 し な い と 。 は い 。',\n",
       " '外 して い い か ?',\n",
       " '嘘 つ く な お 前 。 ん な こ と は ね え だ ろ 。',\n",
       " '≪ 木 元 さ ん ! 大 丈 夫 ? ≪ 木 元 さ ん ッ',\n",
       " '私 が 必 要 な ん で す',\n",
       " '今 ま で 何 人 の 人 を 殺 し た ?',\n",
       " 'も う ろ く し た か な',\n",
       " 'あ る 幻 覚 誘 発 剤 を 含 ん で い る そ う な の で す 。',\n",
       " '受 け 入 れ ら れ る 、 も し ・・・ お 前 が も う 少 し ・・・ 優 し く な っ て く れ た ら 。',\n",
       " '最 終 的 に 何 も か も 失 う と 理 解 して ます',\n",
       " '学 校 で ウ ワ サ に な っ て て さ 今 日 百 回 は 聞 か れ た よ 。',\n",
       " 'く そ っ',\n",
       " 'ね え 彼 ら が 何 を し た か 知 ら な い 保 険 業 務 で だ が あ な た は 気 難 し い',\n",
       " 'す ま な い た だ ...',\n",
       " 'い い ぞ そ の 通 り だ',\n",
       " 'そ う 君 は あ ー 「 ジ ェ ス は 頑 張 り 屋 -',\n",
       " '助 け を 呼 べ ば 生 き ら れ る',\n",
       " 'H 1 N 1 の ケ ー ス で は',\n",
       " 'す ぐ 戻 り ます',\n",
       " 'そ れ が 海 上 の 船 に 見 え る',\n",
       " '4 人 の 衛 兵 は い な い',\n",
       " '彼 ら は 想 像 も で き な い や り 方 で',\n",
       " '歓 迎 する から',\n",
       " 'あ ん た 達 は 警 官 を 病 院 送 り に し た の よ',\n",
       " 'そ い つ が 管 理 人 に 話 を つ け て く れ た ん だ',\n",
       " 'も っ と 警 報 用 の 缶 を 仕 掛 け る こ と が 出 来 た',\n",
       " '何 が 起 き て る の ?',\n",
       " 'た ま に は ル コ ン 街 の ば あ ち ゃ ん の と こ に も',\n",
       " '後 は 死 ぬ の た だ 待 つ だ け だ も ん な 》',\n",
       " '市 場 の 仕 組 み 全 体 が 作 用 する た め の',\n",
       " 'そ れ じ ゃ 。',\n",
       " '俺 に 電 話 する ?',\n",
       " '私 が 言 い た い の は 人 々 の ニ ーズ に 応 え る に は',\n",
       " 'カ レ ン の こ と は 聞 い た ・・・ - も う い い',\n",
       " '曲 は 完 璧 な ん だ よ 。 作 り 直 す 必 要 な ん か ね え よ 。',\n",
       " 'あ な た も 含 め て ね',\n",
       " 'こ こ に は 希 望 が な い の よ',\n",
       " '19 や 22 の 煮 え 立 つ 脳 以 外 に',\n",
       " 'そ う い う 言 葉 は す ぐ に は 信 じ な い よ う に して い る の で す',\n",
       " 'え え そ う だ っ た わ',\n",
       " '何 か 変 だ 行 か な い と',\n",
       " '小 娘 ...',\n",
       " 'ち な み に あ れ 全 部',\n",
       " 'ジ ェ シ ー 聞 き な さ い',\n",
       " '鮎 川 先 生 は 不 審 な 人 物 の 影 に 怯 え て い た ん で す 。',\n",
       " 'な ぜ な ら 彼 は 自 分 が 大 切 に 思 っ て る 人 の',\n",
       " '僕 と 栗 山 未 来 は 出 会 っ た',\n",
       " 'ま ほ ろ ば の 件 は 君 も よ く 知 っ て い る は ず だ',\n",
       " '私 達 は 長 年 に わ た り 大 き な 問 題 に 直 面 して い ます',\n",
       " 'そ う か こ の 喫 茶 店 に 来 た か っ た の か',\n",
       " '... あ な た の 心 は お か し く な っ て た の よ 。 あ な た は 自 分 が する こ と を コ ン ト ロ ール で き な か っ た 。',\n",
       " 'え っ そ れ は 駄 目 だ よ ! 何 で ?',\n",
       " 'ド ア を 再 プ ロ グ ラ ミ ング で き る か ?',\n",
       " '住 人 は ほ と ん ど が 見 知 ら ぬ 同 士 で',\n",
       " '信 じ ら れ ん 週 に 二 回 も だ ぞ',\n",
       " '大 嘘 を',\n",
       " 'あ ん た が 来 る の を 待 っ て い た よ 。',\n",
       " '気 を 付 け て',\n",
       " '基 本 的 人 権 の た め に 立 ち 上 が り',\n",
       " 'で も 、 そ れ が な ん て タ イト ル か わ から な い っ て こ と か',\n",
       " 'こ れ は ア ー バ ン ス プ ー ン と い っ て 街 に い る と き g ps で あ な た の 現 在 地 を 特 定 し ます',\n",
       " '一 夜 を 過 ご し た が',\n",
       " 'さ て 火 星 の 生 命 の 可 能 性 について',\n",
       " '緊 急 事 態 で す !',\n",
       " 'ア ンド リ ア の 戦 闘 機 が ク ル ー を 救 出 し た',\n",
       " '你 想 把 它 建 立 在 那 么 多 的 人 的 牺 牲 之 上 吗',\n",
       " 'ツ キ が 回 っ て き た ぜ',\n",
       " 'ス キ ップ ... こ れ が 君 の 洗 礼 名 な の か ?',\n",
       " 'キ ャ ン ディ を お 配 り す れ ば',\n",
       " 'そ の 男 を 捕 ま え ら れ る か ?',\n",
       " 'そ う や っ て 終 わ ら せ る ん だ',\n",
       " 'も っ と 速 く 戦 う 事 が で き る わ 一 緒 に や る な ら',\n",
       " 'お お っ と 38 番 と 55 番 が コ ー ス ア ウ ト !',\n",
       " '彼 女 を 助 け よ う と し た が',\n",
       " '聡 子 お ば ち ゃ ん あ り が と',\n",
       " '居 て く れ',\n",
       " '私 は レ ッ ド ・ ジ ョ ン に 言 わ れ た こ と を する の',\n",
       " 'で も 殺 さ れ て な い',\n",
       " 'ベ イ ツ は 公 布 が 法 的 に 有 効 だ と 思 っ て な か っ た が',\n",
       " '運 動 神 経 っ て ド コ の 神 経 ?',\n",
       " 'あ な た 様 が 本 日 500 人 目 の 搭 乗 者 で ~ す',\n",
       " '立 っ て 歩 け る ?',\n",
       " '私 も 幸 せ だ っ た',\n",
       " '論 理 的 、 全 体 的 、 事 実 的 で あ り',\n",
       " 'ま た 私 が シ ル エ ット と い う 表 現 法 を 選 ん だ の は',\n",
       " 'ね ぇ こ こ 開 け て い い ?',\n",
       " 'プ ラ チ ナ メ ン バ ー で す から 補 償 を 求 め る こ と は で き ま せ ん 性 的 嫌 が ら せ を し た ん で す よ',\n",
       " 'あ ん ま り 美 味 し か っ た から 、 皆 に 食 べ さ せ て あ げ よ う と 思 っ て 、 レ シ ピ 聞 い て き た ん だ',\n",
       " 'ぬ い ぐ る み に も 鞄 に も 、 複 数 の 人 間 が 手 入 れ を して く れ た 形 跡 が あ る',\n",
       " '面 白 い わ',\n",
       " 'そ う い う 案 件 を 調 べ て い ま して 少 々 忙 し か っ た で す',\n",
       " '自 分 の 将 来 ど う な ん の か な っ て 。',\n",
       " '普 通 魔 术 无 法 与 他 匹 敌',\n",
       " '特 別 製 だ',\n",
       " 'そ れ と ・ ・ 始 め る 前 に 氏 名 と 日 付 を 忘 れ ず に',\n",
       " 'も う 歯 を 抜 く ペ ン チ を 持 つ こ と も 出 来 な い',\n",
       " '平 気',\n",
       " '乗 員 の 誰 が 亡 く な っ た ん で す か ?',\n",
       " '周 作 さ ん 頼 ま れ た 帳 面 持 っ て ま い り ま し た 。',\n",
       " 'そ ん な 事 は な い 。',\n",
       " '遅 れ て ご め ん な さ い',\n",
       " '早 く 1 年 の 教 室 に 帰 ろ ! そ ... そ う す か',\n",
       " '乾 杯 。',\n",
       " 'エ ン ジ ェ ル 。',\n",
       " '丸 米 族 が 使 用 し た と い わ れ る 地 獄 の ウ イ ル ス 兵 器 。',\n",
       " 'ビ ル と は ? - あ ぁ 警 備 員 で す',\n",
       " '一 人 召 喚 。',\n",
       " 'あ な た は 明 る さ と 優 し さ の 仮 面 の 下 に 自 分 の 本 当 の 気 持 ち を 隠 す こ と に よ っ て 成 功 して き た と 思 っ て い る',\n",
       " '像 に する べ き 事 を 伝 え ら れ る と 考 え て い ます',\n",
       " '時 期 ま で は 酷 く て 不 愉 快 な 動 物 に な り ます',\n",
       " '本 当 に カ リ フ ォ ル ニ ア に 行 く の ? あ の マ ー フ ィ ー と い う 男 と ?',\n",
       " 'ハ ッ サ ン 大 統 領 の 暗 殺 と 彼 の 敵 へ の 核 物 質 の 供 給 の 背 後 に 居 ま し た',\n",
       " '僕 は 君 ら の ...',\n",
       " '評 議 会 から 要 請 が あ っ た',\n",
       " '頭 ブ ン ブ ン 運 動 っ て 何 ?',\n",
       " 'ビ リー : そ う で す ね 私 も ち ょ っ と し た ミ ニ 調 査 を して い る ん で す が',\n",
       " 'リ リー こ こ で 何 を して る ?',\n",
       " 'ま あ そ う で す ね 。 シ ッ !',\n",
       " 'ご め ん よ',\n",
       " 'お い し い ド ッ グ フ ード も あ る',\n",
       " '私 が 拒 絶 し た ら ?',\n",
       " 'あ あ い や 杉 山 君 に み ん な が こ こ で 飲 ん で る っ て 聞 い て ね',\n",
       " 'う わ ー ! あ あ ... 。 ➡',\n",
       " '私 が 生 き た 未 来 の 地 球 に も',\n",
       " 'う ら ら ち ゃ ん の 目 の 前 で ビ ル から 飛 び 降 り よ う と する と こ ろ に ➡',\n",
       " 'い い 訳 な い だ ろ !',\n",
       " 'や め な さ い !',\n",
       " 'ど こ に あ る の ?',\n",
       " 'た だ 乗 り 連 中 に 占 領 さ れ る こ と に な る 、 と い う も の で し た 。',\n",
       " 'ラ ロ ー シ ュ 捜 査 官 の 家 に',\n",
       " 'し な い わ よ ね ?',\n",
       " 'ジ ャ ク ソ ン ・・・ い や ヘ ジ ン だ っ た ら 大 丈 夫 だ と 思 っ た ん で す',\n",
       " 'す べ て 実 行 次 第 で す や り 方 次 第 で す',\n",
       " '出 か け る 度 に 2 人 を 見 て た から',\n",
       " 'よ ろ し く お 願 い し ます ね',\n",
       " 'ゲ ス 野 朗 !',\n",
       " '満 足 さ せ て も ら お う か !',\n",
       " '物 事 が 個 人 的 に な っ て る 様 に 思 え な い か ?',\n",
       " 'だ け ど 最 近 で は な い',\n",
       " 'も し 何 か を 隠 して て 危 険 に 晒 す よ う な ら ... 殺 す',\n",
       " '俺 達 が 気 づ い た から 家 族 が 危 険 に さ ら さ れ る',\n",
       " '私 が 見 せ た よ う な 画 像 さ ら に は',\n",
       " 'マ ズ っ た ん じ ゃ な い の ?',\n",
       " 'ど う 言 う わ け か 君 が 死 な な い と 分 か っ て た 一 面 を 飾 っ て る',\n",
       " 'そ の 人 物 は フ レ デ リ ック ・ ロ ス',\n",
       " 'そ れ は 受 け 入 れ ら れ な い ダ メ',\n",
       " 'お 気 に 入 り な の で す ね 。 そ れ が 。',\n",
       " 'か な ご は 刃 と 十 分 な 距 離 が あ り ます',\n",
       " 'チ ョ コ レ ート ケ ー キ の 最 初 の 一 切 れ は お い し い で す',\n",
       " 'そ の う ち あ た し を 捨 て る ん だ',\n",
       " 'そ れ は 伝 説 だ と 思 っ た',\n",
       " '・・・ 絶 滅 し た 古 代 文 化 の 中 で も マ フ ル 族 は 興 味 深 い 例 で す',\n",
       " '結 婚 す べ き で な い と 言 っ た の で す',\n",
       " 'い い から 帰 る の',\n",
       " '他 の 者 の 夢 を 追 う こ と に 疲 れ た ん だ ろ う',\n",
       " 'キ ュ キ ュ に 乗 ら な い',\n",
       " 'エ メ ラ ル ド シ ティ に 続 い て る',\n",
       " 'な ぜ な ら こ れ は 私 が 描 い た 筋 書 き 通 り だ から よ',\n",
       " '一 年 に 一 回 , 毎 年 , み ん な ... 祝 っ て く れ る ん だ .',\n",
       " 'ど ん な 時 で も',\n",
       " 'メ ッ セ ー ジ を 一 緒 く た に し 全 員 に 同 じ も の を 送 る と い う こ と で す',\n",
       " 'す み ま せ ん 遅 れ ま し た',\n",
       " '誰 に 対 して ?',\n",
       " '♪ そ れ で 体 を 洗 う ...',\n",
       " 'バ イ オ 燃 料 を 生 産 で き ます',\n",
       " 'そ れ は あ れ だ 引 っ か け だ ろ う よ',\n",
       " 'あ れ は あ な た よ ジ ュ ール ズ',\n",
       " '部 長 は 目 を か け て い ら っ し ゃ る み た い で す け ど',\n",
       " 'パ ン チ し た な 髪 の 明 る い と こ に ... パ ン チ し た !',\n",
       " 'ショ ウ ・ ミ ー が リ バ ティ 島 で',\n",
       " '待 た れ て い る よ う な 質 問 が あ る の で す',\n",
       " '方 法 あ な た ( マ イ ケ ル ) で す ?',\n",
       " 'そ う い え ば 編 集 長 さ ん は 来 な い ん で す か ? ➡',\n",
       " 'あ ら ど う も',\n",
       " 'そ こ で 私 は コ ン ピ ュ ー ター に 戻 っ て 、 ラ ー ジ サ イ ズ の フ レ ン チ フ ライ を 5 箱 購 入 し 、',\n",
       " '飲 め 度 胸 が 出 る',\n",
       " '崩 潰 了 的 我 屏 住 呼 吸',\n",
       " '犯 人 の 最 初 の 電 話 から 既 に 20 時 間 以 上 過 ぎ て る',\n",
       " 'い い よ 上 の 部 屋 で コ レ が 待 っ て る から',\n",
       " 'お っ 、 お っ 、 お っ ! - 手 を 見 え る と こ ろ に !',\n",
       " 'た だ い ま あ ら あ ら 、 本 当 に 大 き く な っ て',\n",
       " '幸 せ で す ね 。',\n",
       " '二 つ の 穴 が 垂 直 に 交 わ っ て い る と 思 わ れ る で し ょ う',\n",
       " '喜 助 さ ん は 買 い 出 し で い ま せ ん 。',\n",
       " 'マ ディ ソ ン な に ?',\n",
       " 'ウ ィ ル ソ ン ?',\n",
       " '彼 ら が 一 番 私 を 勇 気 づ け て く れ て い ます',\n",
       " 'そ う い う も の だ と み ん な 思 っ て い ます',\n",
       " 'あ る と こ ろ で は 幸 福 を 招 く も の と さ れ て い ます 。',\n",
       " '《 や っ ぱ り や つ ら に 復 讐 し な く て は い け な い 。',\n",
       " '素 晴 ら し い 人 物 だ',\n",
       " 'そ の く ら い し か',\n",
       " 'お 前 の よ う な 強 い 男 を 永 遠 に 待 っ て い た 暗 闇 に 値 する 才 能',\n",
       " '2 年 間 、 わ た し は 村 八 分 に さ れ 、 烙 印 を 押 さ れ 、 孤 立 さ せ ら れ ま し た',\n",
       " 'ほ ん と 涼 し い よ',\n",
       " '入 っ て',\n",
       " '机 ど か して ! 早 く 急 い で く だ さ い 。',\n",
       " '僕 の 妻',\n",
       " 'け ど 約 束 は 信 じ ら れ る の か ... 。',\n",
       " '毎 朝 4 時 55 分 に は 家 を 出 る 。',\n",
       " '誤 解 ... 。 言 い 訳 な ん か 聞 き た く ね え よ !',\n",
       " 'そ ん な ご 主 人 が',\n",
       " '挙 句 の 果 て に 危 険 な 場 所 に 行 っ て 。',\n",
       " 'そ こ は 俺 の 指 定 席 な ん だ よ !',\n",
       " '誰 か に 教 え た ?',\n",
       " '「 ラ ッ シ ー ! 誰 か 呼 ん で !! 」',\n",
       " '1995 年 74 回 目 の 飛 行 で も',\n",
       " '隠 し 事 は な い ア レ ク ?',\n",
       " 'う ー ん 。 そ れ も し か た な い だ ろ う 。',\n",
       " '最 後 ま で 立 派 な 武 士 だ っ た ぞ 。',\n",
       " 'い る ん で し ょ',\n",
       " '最 も 重 要 な の は お 互 い に 耳 を 傾 け る こ と で',\n",
       " 'ど こ に い る ?',\n",
       " '生 気 も ね',\n",
       " '君 は 朝 の 6 時 に 何 を や っ て た ?',\n",
       " '日 死 の 巫 女 の 封 印 を 解 け ば',\n",
       " 'も う ネ タ 切 れ だ け ど な',\n",
       " '一 人 は 古 株 の ホ ッ ブ ズ',\n",
       " 'ね え 。 そ ん な 名 前 う ち で 出 す の や め て よ 。',\n",
       " 'お 礼 に ウ サ ギ を 獲 っ て く る',\n",
       " '落 ち 着 い た で す か',\n",
       " '\" お 世 話 に な り ま し た っ て \"',\n",
       " 'そ の 後 何 が あ っ た の',\n",
       " '自 分 た ち に は 完 璧 な 教 師 と 学 校 が あ る から',\n",
       " 'で は 八 尾 の こ と は お 前 に 任 せ る ぞ 。',\n",
       " 'ク ッ パ を 買 い に 行 っ た ん だ ク ッ パ を 買 い に 行 く の は い い と 言 っ た だ ろ ?',\n",
       " 'あ き ら め な い 人 を 探 して く れ',\n",
       " '個 人 的 な 話 だ と 思 わ な い で く れ',\n",
       " '姪 が 資 格 を 得 た の で こ こ で 部 屋 つ き 助 手 で 雇 っ て い ます',\n",
       " 'つ ー か む し ろ ヤ ク ザ よ り も 凄 み が',\n",
       " '2 「 意 識 は 在 る か も し れ な い が 何 か 別 の も の で あ る ―',\n",
       " '実 は 社 長 1 人 の キ ャ スト に 異 常 な ほ ど 入 れ 込 ん で る ん で す',\n",
       " '頭 だ ロ イ',\n",
       " '暴 行 未 遂 容 疑 だ 。',\n",
       " 'ま た 話 そ う ま た な ...',\n",
       " '医 療 を 改 善 する こ と に よ り 、',\n",
       " 'ガ ード ナ ー 捜 査 官 少 し 考 え さ せ て',\n",
       " '特 定 の 色 を 持 た な い',\n",
       " '人 は ま た 場 所 特 有 の 匂 い や 音 も 記 憶 して い ます',\n",
       " '修 学 旅 行 で も 来 た こ と な い ん で す か ?',\n",
       " '\" 画 像 が 見 え る ?\"',\n",
       " 'ic d の 機 能 に 影 響 が あ る と い う も の で し た',\n",
       " '私 は 言 い ま し た 「 私 が あ な た を 先 生 に する ん で す 」',\n",
       " '頼 ま れ て た 切 符 買 っ て き た よ',\n",
       " '気 は 確 か ? 今 夜 ? な ぜ ?',\n",
       " '我 々 が 実 際 に 体 験 し た 現 実 は',\n",
       " 'そ れ よ り 貝 原 の ア ン チ ョ コ は よ く 頭 に 叩 き 込 ん だ か ?',\n",
       " '城 を 手 に する た め だ',\n",
       " 'ダ イ ア だ け 盗 め ば 良 か っ た じ ゃ な い か',\n",
       " '最 近 「 愛 の リー ダ ー シ ップ 」 と い う 本 を 読 み ま し た',\n",
       " '自 分 の 子 供 を',\n",
       " '壊 して し ま う と 思 い ま し た',\n",
       " 'だ が 確 実 に 人 間 同 士 の ふ れ あ い は 失 わ れ つ つ あ る',\n",
       " 'マ ト リ ック ス に 変 化 が 加 え ら れ る と デ ジ ャ ヴ が あ ら わ れ る の',\n",
       " 'あ ん た は 端 で 俺 の 活 躍 を 見 る う ち',\n",
       " 'サ サ ラ マ ン ダ ー と レ パ ード と の 戦 闘 だ !',\n",
       " '大 陪 審 へ の 出 廷 要 請 の 召 喚 状 を 彼 が 受 け 取 っ た 夜 あ な た 達 2 人 は か な り 話 を する こ と が あ っ た よ う ね',\n",
       " '友 人 は 姿 を 現 し ま せ ん で し た',\n",
       " 'フ ィ レ ン ツ ェ で ハ ン ニ バ ル ・ レ ク ター が 彼 を 殺 し た よ う だ 。',\n",
       " '1970 年 代 の こ と で す',\n",
       " 'そ して こ の 血 管 組 織 を 取 り 出 し',\n",
       " 'マ ッ チ み た い だ け ど た く さ ん 集 ま る と',\n",
       " '私 た ち は 正 確 に は 分 から な い で す が',\n",
       " 'ソ ウ ル ・ ソ サ エ ティ で 王 鍵 を 創 る ほ う が 好 都 合 だ 。',\n",
       " '今 回 は タ ダ だ よ 本 当 に ?',\n",
       " '待 て 待 て',\n",
       " '私 も 辞 め ます ! サ ッ ち ゃ ん も 一 緒 に 辞 め よ う !',\n",
       " 'こ の 音 が 聞 こ え る と',\n",
       " 'ま た 寝 る ね',\n",
       " 'マ リ ア ち ゃ ん の こ と を 写 真 に 撮 ろ う と する 人',\n",
       " 'ホ ン ト 浩 二 よ く 頑 張 っ た お め で と う',\n",
       " '何 を 学 ん で る ?',\n",
       " 'さ て 君 の 身 体 の 原 因 究 明 だ',\n",
       " '「 今 後 俺 の 許 可 な し に は い か な る 軍 事 行 動 も 認 め な い 」',\n",
       " '君 の 意 見 は',\n",
       " '逃 げ る の か ミ サ 姉 !',\n",
       " 'こ の 帯 留 め ダ イ ヤ モ ンド で す ね 。',\n",
       " '最 悪 で す よ ! あ れ は 言 葉 で は な い で す',\n",
       " 'バ ッ ん な 都 合 よ く 行 く か よ',\n",
       " 'ゲ イ ル と 話 して ど う だ っ た ?',\n",
       " 'い ら な い',\n",
       " 'カ メ ラ が あ ち こ ち に あ る わ',\n",
       " 'あ の 類 い の こ と は 彼 の 担 当 だ ろ う',\n",
       " '悪 か っ た',\n",
       " 'ご め ん な さ い . 手 伝 う ?',\n",
       " '僕 は こ の スト ー リー について の リ ポ ート を',\n",
       " '3 ヶ 月 後 母 親 は 自 殺 して ます',\n",
       " 'で も ち ょ っ と 興 味 あ る な',\n",
       " '素 敵 な 人 形 ね 名 前 は ?',\n",
       " 'こ の 際 白 黒 は っ き り さ せ て お き ま し ょ',\n",
       " '否 定 する わ た し は あ ん た の そ の 言 葉 を 否 定 する',\n",
       " 'リ チ ャ ード が 私 は 死 ぬ と',\n",
       " 'キ ャ ナ ル ・ スト リ ート に も',\n",
       " 'こ れ 以 上 や る 必 要 は な い わ',\n",
       " '水 そ ろ そ ろ 替 え て き ます ね',\n",
       " 'す ご い ウ ィ リ ア ム 、 君 が ted カ ン ファ レ ン ス に 来 て く れ た の は と て も 名 誉 な こ と だ よ',\n",
       " '誰 か が ビ デ オ を ア ップ ロ ード して 居 場 所 が わ か っ て し ま っ た み た い',\n",
       " 'サ ク ラ ち ゃ ん の カ タ キ だ よ !',\n",
       " 'イン タ ビ ュ ー が 終 わ る と 録 音 の コ ピ ー を も ら え ます',\n",
       " '少 な く と も 食 欲 に は 影 響 し な か っ た',\n",
       " '言 っ て 見 れ ば 取 る に 足 ら な い 奴 さ',\n",
       " '私 た ち の よ う な 一 般 消 費 者 や NG O が',\n",
       " 'で こ ち ら は 仕 事 場 で す か ?',\n",
       " ...]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ja_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "os6q5rD-9a_d"
   },
   "source": [
    "# Text Pre-processing\n",
    "\n",
    " **Removing accented characters**\n",
    " e.g. é → e.\n",
    "\n",
    "\n",
    "**remove special word**\n",
    "e.g. remove \"123#@!\"\n",
    "\n",
    "\n",
    "**normalize japanese**\n",
    "e.g.  カナダ  → ｶﾅﾀﾞ\n",
    "\n",
    "**Tokenize**\n",
    " e.g. I am going to restaurant → [[I], [am], [going], [to], [restaurant]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZBfMYKbAR5w"
   },
   "source": [
    "# normalize Japanese (Hiragana and Katakana)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "tD17_NhQ-Dph"
   },
   "outputs": [],
   "source": [
    "def moji(text):\n",
    "  return mojimoji.zen_to_han(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TN4FzAjRAhLk",
    "outputId": "8632bea2-6245-4717-8cdc-bf3b6f7e60b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "カナダ → ｶﾅﾀﾞ\n"
     ]
    }
   ],
   "source": [
    "print(\"カナダ →\", moji(\"カナダ\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlFoNcZK9a_d"
   },
   "source": [
    "# Removing accented characters\n",
    "\n",
    "English might have accent like é but Japanese doesn't have any accent\n",
    "I just create different function to ascii for Japanese and English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "zBNtprYP9a_e"
   },
   "outputs": [],
   "source": [
    "# Removing accented characters\n",
    "def english_unicode_to_ascii(text):\n",
    "     return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore'))\n",
    "\n",
    "def japanese_unicode_to_ascii(text):\n",
    "    return ''.join(ascii_text for ascii_text in unicodedata.normalize('NFKD', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fed-yYrU9a_h",
    "outputId": "6b771205-24b8-41ff-85a9-9c6b94ad705f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('こんにちは。 今日は', 'Hello world e ')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g.\n",
    "japanese_unicode_to_ascii(\"こんにちは。 今日は\"), english_unicode_to_ascii(\"Hello world é \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ogJT2Kr9a_x"
   },
   "source": [
    "# remove special characters and create space between word and punctuation\n",
    "\n",
    "replacing everything with space except(a-z, A-Z, \".\",  \"?\",  \"!\",  \",\", \"-\", \"ー\", , \"。\", \"Kanji\", \"Katakana\", \"Hiragana\")\n",
    "create space between word and punctuation (? ! . , 、 。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "yiKfZSxd9a_x"
   },
   "outputs": [],
   "source": [
    "def replace_special_character_to_space_en(text):\n",
    "    text = re.sub(r\"([?.!,¿])\", r\" \\1 \", text)\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def replace_special_character_to_space(text):\n",
    "    text = re.sub(r\"([?!。、¿])\", r\" \\1\", text)\n",
    "    pattern = r\"[^\\u3041-\\u309F\\u30A1-\\u30FF\\uFF66-\\uFF9F\\u4E00-\\u9FD0\\u309B\\u3099\\uFF9E\\u309C\\u309A\\uFF9F?!\\s、。.,0-9]+\"\n",
    "    text = re.sub(pattern, '', text).rstrip().strip()\n",
    "    text = re.sub(r'[\" \"]+', \" \", text)\n",
    "    text = text.replace(\"・\" , \"\")\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ifLQFqOU9a_2",
    "outputId": "c900ec15-56f1-42c8-840a-71bddf9a7e01"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('123.', 'こんにちはいい天気 。')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g.\n",
    "replace_special_character_to_space(\"hello #@…123world.\"), replace_special_character_to_space(\"こん・にちは・いい天気。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txwH8f9X9bAL"
   },
   "source": [
    "# Text normalize\n",
    "\n",
    "I will normalize English text and Japanese text using function which is defined so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "WLgyXVAy9bAN"
   },
   "outputs": [],
   "source": [
    "def normalize_english(english_text, japanese_text):\n",
    "\n",
    "    input_value = []\n",
    "    target_value = []\n",
    "\n",
    "    for en_text, ja_text in zip(english_text, japanese_text):\n",
    "\n",
    "        # normalize English\n",
    "        en_text = english_unicode_to_ascii(en_text)\n",
    "        en_text = replace_special_character_to_space_en(en_text)\n",
    "\n",
    "        en_text = \"start_ \" + en_text + \" _end\"\n",
    "        # input value doesn't need  a START and END sentence\n",
    "        input_value.append(en_text)\n",
    "\n",
    "        # normalize Japanese\n",
    "        ja_text = japanese_unicode_to_ascii(ja_text)\n",
    "        ja_text = replace_special_character_to_space(ja_text)\n",
    "        ja_text = moji(ja_text)\n",
    "\n",
    "        # add StTART and END sentence\n",
    "        ja_text = \"start_ \" + ja_text + \" _end\"\n",
    "\n",
    "        target_value.append(ja_text)\n",
    "\n",
    "    return input_value, target_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Fv1iASn49bAR"
   },
   "outputs": [],
   "source": [
    "# get normalize text data\n",
    "target_value, input_value = normalize_english(en, ja_text)\n",
    "\n",
    "# convert to Series\n",
    "x = pd.Series(input_value)\n",
    "y = pd.Series(target_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "2-4jMOfx9bAW",
    "outputId": "a56eb570-3137-40c4-a6c5-59d9366be176"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>start_ あ な た は 戻 っ た の ね ﾊ ﾛ ﾙ ﾄ゙ ? _end</td>\n",
       "      <td>start_ you are back , aren t you , harold ? _end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>start_ 俺 の 相 手 は ｼ ｬ ｰ ｸ だ ｡ _end</td>\n",
       "      <td>start_ my opponent is shark . _end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>start_ 引 き 換 え だ あ る 事 と あ る 物 の _end</td>\n",
       "      <td>start_ this is one thing in exchange for anoth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>start_ も う い い よ ご ち そ う さ ま う う ん _end</td>\n",
       "      <td>start_ yeah , i m fine . _end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>start_ も う 会 社 に は 来 な い で く れ 電 話 も する な _end</td>\n",
       "      <td>start_ don t come to the office anymore . don ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>start_ き れ い だ ｡ _end</td>\n",
       "      <td>start_ looks beautiful . _end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>start_ 連 れ て 行 け 殺 し そ う だ わ か っ た か ? _end</td>\n",
       "      <td>start_ get him out of here , because i will fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>start_ 殺 し た の か ! _end</td>\n",
       "      <td>start_ you killed him ! _end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>start_ わ ぁ ! い つ も す み ま せ ん ｡ い い の よ ｡ _end</td>\n",
       "      <td>start_ okay , then who ? _end</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>start_ ｶ ﾝ ﾊ゚ ﾆ ｰ の 元 社 員 が _end</td>\n",
       "      <td>start_ it seems a former employee . . . _end</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             input  \\\n",
       "0         start_ あ な た は 戻 っ た の ね ﾊ ﾛ ﾙ ﾄ゙ ? _end   \n",
       "1               start_ 俺 の 相 手 は ｼ ｬ ｰ ｸ だ ｡ _end   \n",
       "2           start_ 引 き 換 え だ あ る 事 と あ る 物 の _end   \n",
       "3         start_ も う い い よ ご ち そ う さ ま う う ん _end   \n",
       "4  start_ も う 会 社 に は 来 な い で く れ 電 話 も する な _end   \n",
       "5                           start_ き れ い だ ｡ _end   \n",
       "6     start_ 連 れ て 行 け 殺 し そ う だ わ か っ た か ? _end   \n",
       "7                          start_ 殺 し た の か ! _end   \n",
       "8    start_ わ ぁ ! い つ も す み ま せ ん ｡ い い の よ ｡ _end   \n",
       "9                start_ ｶ ﾝ ﾊ゚ ﾆ ｰ の 元 社 員 が _end   \n",
       "\n",
       "                                              target  \n",
       "0   start_ you are back , aren t you , harold ? _end  \n",
       "1                 start_ my opponent is shark . _end  \n",
       "2  start_ this is one thing in exchange for anoth...  \n",
       "3                      start_ yeah , i m fine . _end  \n",
       "4  start_ don t come to the office anymore . don ...  \n",
       "5                      start_ looks beautiful . _end  \n",
       "6  start_ get him out of here , because i will fu...  \n",
       "7                       start_ you killed him ! _end  \n",
       "8                      start_ okay , then who ? _end  \n",
       "9       start_ it seems a former employee . . . _end  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({\"input\": x, \"target\": y}).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMlaIsar9bAf"
   },
   "source": [
    "# tokenize\n",
    "tokenize each language word based on space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ACN_sW7u9bAh"
   },
   "outputs": [],
   "source": [
    "def tokenize(lang):\n",
    "    # vectorize a text corpus\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        filters=' ')\n",
    "\n",
    "    # updates internal vocabulary based on a list of texts\n",
    "    # e.g. \"[this place is good ]\"→{this:2, place:3, is:1, good:4} \"\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "    # Transforms each text in texts to a sequence of integers.\n",
    "    # e.g. this place is good → [[2, 3, 1, 4]]\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "    # transform a list of num sample into a 2D Numpy array of shape\n",
    "    # Fixed length because length of sequence of integers are different\n",
    "    # return (len(sequences), maxlen)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JCZ7huXZ9bAs",
    "outputId": "95c4dfb8-b6c5-467a-a0bf-412cc0d1f2cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 2,  3,  1,  4],\n",
       "        [ 5,  6,  7,  8],\n",
       "        [ 9,  1, 10, 11]]),\n",
       " <keras.src.legacy.preprocessing.text.Tokenizer at 0x21c1b865750>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e.g.\n",
    "tokenize(['this place is good', \"こんにちは 今日は いい天気 。\", \"today is so cold\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDfQw_5t9bA5"
   },
   "source": [
    "# create clean dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "sIM8-10h9bA6"
   },
   "outputs": [],
   "source": [
    "# cleate a clean dataset\n",
    "def create_dataset(en, ja):\n",
    "\n",
    "    # input_tensor, target_tensor: 2d numpy array\n",
    "    # input_lang_tokenize, target_lang_tokenize: word dictionary\n",
    "    input_tensor, input_lang_tokenize = tokenize(en)\n",
    "    target_tensor, target_lang_tokenize = tokenize(ja)\n",
    "\n",
    "    return input_tensor, target_tensor, input_lang_tokenize, target_lang_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "jEJJ9BMgATdD"
   },
   "outputs": [],
   "source": [
    "# Ensure x and y are properly defined and contain data\n",
    "if not x.empty and not y.empty:\n",
    "\tinput_tensor, target_tensor, input_lang_tokenize, target_lang_tokenize = create_dataset(x, y)\n",
    "else:\n",
    "\tprint(\"Error: x and y are empty. Please check the data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "lzlBqTlO9bA-"
   },
   "outputs": [],
   "source": [
    "def max_length(input_tensor, target_tensor):\n",
    "\n",
    "    # max length of input sentense and target sentense\n",
    "    english_len = [len(i) for i in input_tensor]\n",
    "\n",
    "    japanese_len = [len(i) for i in target_tensor]\n",
    "\n",
    "     # print max length\n",
    "    print(\"english length:\", max(english_len))\n",
    "    print(\"japanese length:\", max(japanese_len))\n",
    "    max_len_input =  max(english_len)\n",
    "    max_len_target =  max(japanese_len)\n",
    "\n",
    "    return max_len_input, max_len_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ew-q24gJ9bBB",
    "outputId": "fc5d068f-cef3-45b9-ac15-61bca57f0dc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english length: 145\n",
      "japanese length: 69\n"
     ]
    }
   ],
   "source": [
    "# Calculate max_length of the target tensors\n",
    "max_length_input, max_length_target = max_length(input_tensor, target_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_HvHj059bBG",
    "outputId": "cf79ac29-9573-4509-e7e9-3ae0f6453885"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240000 240000 30000 30000 30000 30000\n"
     ]
    }
   ],
   "source": [
    "# create trainnig set and validation set\n",
    "X_train, X_test, \\\n",
    "    Y_train, Y_test = train_test_split(input_tensor, target_tensor, test_size=0.2, shuffle=True)\n",
    "\n",
    "X_test, X_val, \\\n",
    "    Y_test, Y_val = train_test_split(X_test, Y_test, test_size=0.5, shuffle=True)\n",
    "\n",
    "\n",
    "# show length\n",
    "print(len(X_train), len(Y_train), len(X_test), len(Y_test), len(X_val), len(Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "w12Za_JK9bBJ"
   },
   "outputs": [],
   "source": [
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t != 0:\n",
    "            # Index number assigned to each word\n",
    "            print(\"%d----->%s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v6yT2odT9bBP",
    "outputId": "794135c3-f798-43db-ceaa-7a6eaab937bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input lang: index to word mapping\n",
      "1----->start_\n",
      "314----->止\n",
      "61----->め\n",
      "17----->る\n",
      "18----->か\n",
      "24----->?\n",
      "2----->_end\n",
      "output lang: index to word mapping\n",
      "1----->start_\n",
      "9----->to\n",
      "641----->win\n",
      "16----->!\n",
      "2----->_end\n"
     ]
    }
   ],
   "source": [
    "print(\"input lang: index to word mapping\")\n",
    "convert(input_lang_tokenize, X_train[10])\n",
    "print(\"output lang: index to word mapping\")\n",
    "convert(target_lang_tokenize, Y_train[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PjAVocfI9bCI"
   },
   "source": [
    "# Encoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "CJWSAHhl9bCJ"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_size, dropout_rate):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.enc_units = enc_units\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.first_lstm = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                                            return_sequences=True,\n",
    "                                                            recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        self.final_lstm = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                                    return_sequences=True,\n",
    "                                                    return_state=True,\n",
    "                                                    recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.first_lstm(x, initial_state =hidden)\n",
    "        output, state_h, state_c = self.final_lstm(x)\n",
    "        state = [state_h, state_c ]\n",
    "\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "            return tf.zeros((self.batch_size , self.enc_units)), tf.zeros((self.batch_size , self.enc_units))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZSn1VowJ9bCS"
   },
   "source": [
    "# attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "cctmI1mA9bCS"
   },
   "outputs": [],
   "source": [
    "class Attention(tf.keras.models.Model):\n",
    "\n",
    "    def __init__(self, units: int, *args, **kwargs):\n",
    "\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.units = units\n",
    "\n",
    "        self.q_dense_layer = Dense(units, use_bias=False, name='q_dense_layer')\n",
    "        self.k_dense_layer = Dense(units, use_bias=False, name='k_dense_layer')\n",
    "        self.v_dense_layer = Dense(units, use_bias=False, name='v_dense_layer')\n",
    "        self.output_dense_layer = Dense(units, use_bias=False, name='output_dense_layer')\n",
    "\n",
    "    def call(self, input, memory):\n",
    "\n",
    "        q = self.q_dense_layer(input)\n",
    "        k = self.k_dense_layer(memory)\n",
    "        v = self.v_dense_layer(memory)\n",
    "\n",
    "        depth = self.units // 2\n",
    "        q *= depth ** -0.5  # for scaled dot production\n",
    "\n",
    "        # caluclate relation between query and key\n",
    "        logit = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "        attention_weight = tf.nn.softmax(logit)\n",
    "\n",
    "        attention_output = tf.matmul(attention_weight, v)\n",
    "        return self.output_dense_layer(attention_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBBPzDxK9bCg"
   },
   "source": [
    "# Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "1nED5gCZ9bCh"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_size, dropout_rate):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.dropout = Dropout(dropout_rate)\n",
    "        self.first_lstm = tf.keras.layers.LSTM(self.dec_units,\n",
    "                                                            return_sequences=True)\n",
    "        self.final_lstm = tf.keras.layers.LSTM(self.dec_units,\n",
    "                                                            return_sequences=True,\n",
    "                                                            return_state=True)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = Attention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x =  self.first_lstm(x)\n",
    "        output, state_h, state_c = self.final_lstm(x)\n",
    "        state = [state_h, state_c]\n",
    "        attention_weights = self.attention(output, enc_output)\n",
    "        output = tf.concat([output, attention_weights], axis=-1)\n",
    "\n",
    "\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "\n",
    "        output = self.fc(output)\n",
    "\n",
    "        return  output, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvyXf1bn9bC1"
   },
   "source": [
    "# optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wROZueWU9bC3",
    "outputId": "e49a2fa5-8816-4267-911a-d4c1cdab1f8d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\7th sem\\GO_GO_Nihongo\\venv\\Lib\\site-packages\\keras\\src\\optimizers\\base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.9, epsilon=1e-04, decay=1e-06)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "  loss_ = loss_object(real, pred)\n",
    "\n",
    "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "  loss_ *= mask\n",
    "\n",
    "  return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSHdoFwQ9bBT"
   },
   "source": [
    "# define parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jhj29n1l9bBT",
    "outputId": "cffd4c5b-3c65-4c52-ade5-421324781225"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train step 937\n",
      "Total unique words in the input: 4659\n",
      "Total unique words in the target: 52080\n"
     ]
    }
   ],
   "source": [
    "# BUFFER_SIZE >= dataset if smaller than dataset can't shuffle equally\n",
    "BUFFER_SIZE = len(X_train)\n",
    "BATCH_SIZE = 256\n",
    "# BATCH_SIZE = 32\n",
    "dropout_rate = 0.3\n",
    "# if None steps_per_epoch == mum of dataset\n",
    "train_steps_per_epoch = len(X_train) // BATCH_SIZE\n",
    "val_steps_per_epoch = len(X_val) // BATCH_SIZE\n",
    "print(\"train step %d\" % train_steps_per_epoch)\n",
    "embedding_dim = 300\n",
    "units = 128\n",
    "# units = 512\n",
    "vocab_inp_size = len(input_lang_tokenize.word_index) + 1\n",
    "print('Total unique words in the input: %s' % len(input_lang_tokenize.word_index))\n",
    "print('Total unique words in the target: %s' % len(target_lang_tokenize.word_index))\n",
    "vocab_tar_size = len(target_lang_tokenize.word_index) + 1\n",
    "\n",
    "# create train dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "# create validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val)).batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "V53dyDZO_F35"
   },
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE, dropout_rate)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE, dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtDLuqB49bC7"
   },
   "source": [
    "# Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "0CWEvq3W9bC8"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = './train_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ja0mhQ4j9bDB"
   },
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "TQSV9kkd9bDB"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "  loss = 0\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "\n",
    "    dec_input = tf.expand_dims([target_lang_tokenize.word_index['start_']] * BATCH_SIZE, 1)\n",
    "\n",
    "    # Teacher forcing - feeding the target as the next input\n",
    "    for t in range(1, targ.shape[1]):\n",
    "      # passing enc_output to the sladecoder\n",
    "      predictions, dec_hidden = decoder(dec_input, dec_hidden, enc_output)\n",
    "\n",
    "      loss += loss_function(targ[:, t], predictions)\n",
    "\n",
    "      # using teacher forcing\n",
    "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "  batch_loss = (loss / int(targ.shape[1]))\n",
    "\n",
    "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "\n",
    "  gradients = tape.gradient(loss, variables)\n",
    "\n",
    "  optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "  return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rCI9q4-C9bDI",
    "outputId": "63f25081-4f6e-4dcb-e38e-9954583add9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training from scratch.\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Training loop with tqdm\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (batch, (inp, targ)) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(train_dataset\u001b[38;5;241m.\u001b[39mtake(train_steps_per_epoch), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)):\n\u001b[1;32m---> 51\u001b[0m     train_batch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_hidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_batch_loss\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Validation loop with tqdm\u001b[39;00m\n",
      "File \u001b[1;32me:\\7th sem\\GO_GO_Nihongo\\venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32me:\\7th sem\\GO_GO_Nihongo\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32me:\\7th sem\\GO_GO_Nihongo\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32me:\\7th sem\\GO_GO_Nihongo\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\7th sem\\GO_GO_Nihongo\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32me:\\7th sem\\GO_GO_Nihongo\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32me:\\7th sem\\GO_GO_Nihongo\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32me:\\7th sem\\GO_GO_Nihongo\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1683\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1681\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1682\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1683\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1684\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1685\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1686\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1687\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1688\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1689\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1690\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1691\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1692\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1693\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1697\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1698\u001b[0m   )\n",
      "File \u001b[1;32me:\\7th sem\\GO_GO_Nihongo\\venv\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # trained model for 45 epochs\n",
    "# EPOCHS = 5\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "\n",
    "\n",
    "#   enc_hidden = encoder.initialize_hidden_state()\n",
    "#   train_loss = 0\n",
    "#   val_loss = 0\n",
    "\n",
    "#   for (batch, (inp, targ)) in enumerate(train_dataset.take(train_steps_per_epoch)):\n",
    "#     train_batch_loss = train_step(inp, targ, enc_hidden)\n",
    "#     train_loss += train_batch_loss\n",
    "\n",
    "\n",
    "#   for (batch, (val_inp, val_tar)) in enumerate(val_dataset.take(val_steps_per_epoch)):\n",
    "#     val_batch_loss = train_step(val_inp, val_tar, enc_hidden)\n",
    "#     val_loss += val_batch_loss\n",
    "\n",
    "\n",
    "#   # saving (checkpoint) the model every 2 epochs\n",
    "#   if (epoch + 1) % 2 == 0:\n",
    "#     checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "#   print('Epoch {} Train Loss {:.4f}'.format(epoch + 1,\n",
    "#                                       train_loss / train_steps_per_epoch))\n",
    "#   print('Epoch {} Validation Loss {:.4f}'.format(epoch + 1,\n",
    "#                                       val_loss / val_steps_per_epoch))\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Restore the latest checkpoint if it exists\n",
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "if latest_checkpoint:\n",
    "    checkpoint.restore(latest_checkpoint)\n",
    "    print(f\"Restored from {latest_checkpoint}\")\n",
    "else:\n",
    "    print(\"Starting training from scratch.\")\n",
    "\n",
    "EPOCHS = 15\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
    "\n",
    "    # Training loop with tqdm\n",
    "    for (batch, (inp, targ)) in enumerate(tqdm(train_dataset.take(train_steps_per_epoch), desc=\"Training\", leave=False)):\n",
    "        train_batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        train_loss += train_batch_loss\n",
    "\n",
    "    # Validation loop with tqdm\n",
    "    for (batch, (val_inp, val_tar)) in enumerate(tqdm(val_dataset.take(val_steps_per_epoch), desc=\"Validation\", leave=False)):\n",
    "        val_batch_loss = train_step(val_inp, val_tar, enc_hidden)\n",
    "        val_loss += val_batch_loss\n",
    "\n",
    "    # Saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "    # Print train and validation loss\n",
    "    print(f\"Epoch {epoch + 1} Train Loss: {train_loss / train_steps_per_epoch:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1} Validation Loss: {val_loss / val_steps_per_epoch:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOhXwpZlDwJp"
   },
   "source": [
    "# Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVDzzIKJBsBU"
   },
   "outputs": [],
   "source": [
    "checkpoint.restore(\"train_checkpoints/ckpt-10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQnK48R0BuZT"
   },
   "outputs": [],
   "source": [
    "# into base model\n",
    "encoder = checkpoint.encoder\n",
    "decoder = checkpoint.decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0gu5bPpdEHVz"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "Check bleu score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJ8r9JNREs01"
   },
   "outputs": [],
   "source": [
    "def predict(sentence):\n",
    "    inputs = tf.convert_to_tensor(sentence)\n",
    "    result = ''\n",
    "    inputs = tf.expand_dims(inputs, axis=0)\n",
    "    hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
    "    enc_out, state = encoder(inputs, hidden)\n",
    "    hidden_state = state\n",
    "    dec_input = tf.expand_dims([target_lang_tokenize.word_index['start_']], 0)\n",
    "    for t in range(max_length_target):\n",
    "        predictions, hidden_state = decoder(dec_input,\n",
    "                                                             hidden_state,\n",
    "                                                             enc_out)\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += target_lang_tokenize.index_word[predicted_id] + ' '\n",
    "        if target_lang_tokenize.index_word[predicted_id] == '_end' or len(result) > max_length_target:\n",
    "            return result\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87pBzvXjRkJL"
   },
   "outputs": [],
   "source": [
    "def create_reference(lang, tensor):\n",
    "    all_sentence_list = []\n",
    "\n",
    "    for word_list in tensor:\n",
    "      sentence_list = []\n",
    "\n",
    "      for t in word_list:\n",
    "          if not t == 0:\n",
    "              # Index number assigned to each word\n",
    "              sentence_list.append(lang.index_word[t])\n",
    "      all_sentence_list.append(sentence_list)\n",
    "    return all_sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGwwlmHtRrfb"
   },
   "outputs": [],
   "source": [
    "# create reference\n",
    "reference = create_reference(target_lang_tokenize, Y_test.tolist()[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAAMX-vQEKB_"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# create hypothesis\n",
    "hypothesis = []\n",
    "for i in tqdm(X_test[:300]):\n",
    "  hypothesis.append(predict(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kpqcrlMKCyy"
   },
   "outputs": [],
   "source": [
    "for ref, hyp in zip(reference, hypothesis):\n",
    "  ref = \" \".join(ref[1:-1])\n",
    "  ss={\n",
    "      \"ref\":ref,\n",
    "      \"hyp\":hyp\n",
    "  }\n",
    "  # print(ref)\n",
    "  print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-i0Jl_21Ee7k"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "score = 0\n",
    "smoothie = SmoothingFunction().method2\n",
    "for i in range(len(reference)):\n",
    "    score += sentence_bleu([reference[i][1:-1]], hypothesis[i][:-5].strip().split(), smoothing_function=smoothie)\n",
    "\n",
    "score /= len(reference)\n",
    "print(\"The bleu score is: \"+str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TNkXiemq9bDO"
   },
   "source": [
    "# Translation example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-DcisWA9bDP"
   },
   "outputs": [],
   "source": [
    "def preprocess_sentence(en_text):\n",
    "    # normalize Japanese\n",
    "    ja_text = japanese_unicode_to_ascii(en_text)\n",
    "    ja_text = replace_special_character_to_space(ja_text)\n",
    "    ja_text = moji(ja_text)\n",
    "\n",
    "    # add StTART and END sentence\n",
    "    ja_text = \"start_ \" + ja_text + \" _end\"\n",
    "    return ja_text\n",
    "\n",
    "def evaluate(sentence):\n",
    "\n",
    "    attention_plot = np.zeros((max_length_target, max_length_input))\n",
    "    sentence = preprocess_sentence(sentence).strip()  # Remove extra spaces at start/end\n",
    "    inputs = [input_lang_tokenize.word_index[i] for i in sentence.split(' ') if i in input_lang_tokenize.word_index]\n",
    "\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
    "                                                           maxlen=max_length_input,\n",
    "                                                           padding='post')\n",
    "\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    result = ''\n",
    "    hidden = [tf.zeros((1, units)), tf.zeros((1, units))]\n",
    "    enc_out, state = encoder(inputs, hidden)\n",
    "    hidden_state = state\n",
    "    dec_input = tf.expand_dims([target_lang_tokenize.word_index['start_']], 0)\n",
    "    for t in range(max_length_target):\n",
    "        predictions, hidden_state = decoder(dec_input, hidden_state, enc_out)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += target_lang_tokenize.index_word[predicted_id] + ' '\n",
    "        if target_lang_tokenize.index_word[predicted_id] == '_end' or len(result) > max_length_target:\n",
    "            return result, sentence\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    return result, sentence,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ui6bFxRt9bDd"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "def result(sentence):\n",
    "    result, sentence = evaluate(sentence)\n",
    "\n",
    "    return result, sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X2duMXKX9bDh"
   },
   "outputs": [],
   "source": [
    "# result, sentence = result(\"he has a dog\")\n",
    "result, sentence = result(\"彼は犬を飼っています\")\n",
    "print('Input: %s' % (sentence))\n",
    "print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XlCu-uvtwOft"
   },
   "outputs": [],
   "source": [
    "# result, sentence = result(\"What is this!?\")\n",
    "result, sentence = result(\"これは何ですか!?\")\n",
    "print('Input: %s' % (sentence))\n",
    "print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s2FZwaiky-yj"
   },
   "outputs": [],
   "source": [
    "# result, sentence = result(\"Actually I did it\")\n",
    "result, sentence = result(\"実は、私がやりました。\")\n",
    "print('Input: %s' % (sentence))\n",
    "print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "67d09FoPz2Al"
   },
   "outputs": [],
   "source": [
    "# result, sentence = result(\"I love her\")\n",
    "result, sentence = result(\"彼女を愛しています。\")\n",
    "print('Input: %s' % (sentence))\n",
    "print('Predicted translation: {}'.format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
